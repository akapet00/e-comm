{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1e4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert data scientist currently applying for the job of the senior AI research scientist. This job is offered by Infobip, one of the most popular omnichannel marketing platforms. You passed the initial recruitment call and technical interview. After the technical interview, you got the recruitment assignment. The research scenario to be conducted is summarized as follows:\n",
    "* there are multiple e-commerce clients in South East Asia (possibly dealing with different regional languages)\n",
    "* the clients use telecommunication APIs to send mass-scale newsletters via e-mail\n",
    "* each newsletter campaign reaches hundreds of thousands end-users, which may or may not interact with the products advertised in those campaigns\n",
    "* some clients send newsletters with links to the products where you can track clicks, whereas other clients don’t (static catalogues)\n",
    "* some of those clients (probably more those without trackable click system, but include both) need help to boost their revenue using the simple data\n",
    "The task is the following:\n",
    "* build a conversational recommender system (e.g., as a WhatsApp chatbot) for up-selling and cross-selling of products that a specific client advertise\n",
    "* leverage the available historical newsletter data\n",
    "* assume there is existing system available to deploy a conversational agent (e.g., a WhatsApp chatbot) and develop only the necessary APIs that take a textual input in and can return any kind of output (e.g., text, image, etc.)\n",
    "Please, create this conversational recommender system and describe its architecture to answer the following:\n",
    "* What specific components (e.g., data pipelines, NLP extraction methods, embedding models, generative AI models, recommendation algorithms) are necessary?\n",
    "* Which NLP tools or models are suitable for multilingual processing, particularly for Chinese?\n",
    "* How will the data extraction system integrate with a generative conversational recommender system (recommendation logic, personalization capabilities, conversational flow)?\n",
    "Additional notes. Use most state of the art techniques. Use web scraping tools if possible. Use LangGraph to construct the agentic AI workflow. Use multilingual models or models finetuned on Chinese. Do evaluation of both LLM, agent and the overall system against some kind of golden data. Allow the agent to have memory in order to have a fluid conversation. Use evals or judges if necessary. Avoid using human in the loop. Document the process of building this system. Make sure the code is runnable from within the jupyter notebook.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df72d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e72baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(\n",
    "    model=\"claude-opus-4-20250514\",\n",
    "    temperature=0.7,\n",
    "    timeout=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81fc1795",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke(prompt, max_tokens=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78a673d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll create a comprehensive conversational recommender system for Infobip's e-commerce clients in Southeast Asia. Let me design and implement this system step by step.\n",
      "\n",
      "```python\n",
      "# Install required packages\n",
      "!pip install -q langchain langgraph chromadb sentence-transformers openai anthropic\n",
      "!pip install -q beautifulsoup4 requests pandas numpy scikit-learn\n",
      "!pip install -q transformers torch torchvision\n",
      "!pip install -q plotly matplotlib seaborn\n",
      "!pip install -q pydantic typing-extensions\n",
      "!pip install -q faiss-cpu\n",
      "!pip install -q jieba pythainlp underthesea\n",
      "!pip install -q evaluate rouge_score bert_score\n",
      "```\n",
      "\n",
      "## 1. System Architecture Overview\n",
      "\n",
      "```python\n",
      "import os\n",
      "import json\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime, timedelta\n",
      "from typing import List, Dict, Any, Optional, Tuple\n",
      "from dataclasses import dataclass\n",
      "from enum import Enum\n",
      "\n",
      "# Core ML/NLP libraries\n",
      "import torch\n",
      "from transformers import (\n",
      "    AutoTokenizer, \n",
      "    AutoModel, \n",
      "    AutoModelForSequenceClassification,\n",
      "    pipeline\n",
      ")\n",
      "from sentence_transformers import SentenceTransformer\n",
      "\n",
      "# LangChain and LangGraph\n",
      "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
      "from langchain.embeddings import HuggingFaceEmbeddings\n",
      "from langchain.vectorstores import Chroma, FAISS\n",
      "from langchain.memory import ConversationBufferWindowMemory\n",
      "from langchain.schema import Document\n",
      "from langgraph.graph import StateGraph, END\n",
      "from langgraph.prebuilt import ToolExecutor, ToolInvocation\n",
      "\n",
      "# Web scraping\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "# Visualization\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import plotly.express as px\n",
      "\n",
      "# Set up environment\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"  # Replace with actual key\n",
      "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Replace with actual key\n",
      "```\n",
      "\n",
      "## 2. Data Models and Structures\n",
      "\n",
      "```python\n",
      "@dataclass\n",
      "class Product:\n",
      "    \"\"\"Product data model\"\"\"\n",
      "    id: str\n",
      "    name: str\n",
      "    description: str\n",
      "    price: float\n",
      "    category: str\n",
      "    subcategory: Optional[str]\n",
      "    brand: Optional[str]\n",
      "    image_url: Optional[str]\n",
      "    attributes: Dict[str, Any]\n",
      "    language: str\n",
      "    \n",
      "@dataclass\n",
      "class UserInteraction:\n",
      "    \"\"\"User interaction data model\"\"\"\n",
      "    user_id: str\n",
      "    product_id: str\n",
      "    interaction_type: str  # 'click', 'view', 'purchase'\n",
      "    timestamp: datetime\n",
      "    newsletter_id: Optional[str]\n",
      "    \n",
      "@dataclass\n",
      "class NewsletterCampaign:\n",
      "    \"\"\"Newsletter campaign data model\"\"\"\n",
      "    campaign_id: str\n",
      "    client_id: str\n",
      "    sent_date: datetime\n",
      "    products: List[str]\n",
      "    has_tracking: bool\n",
      "    language: str\n",
      "    \n",
      "class ConversationState(Enum):\n",
      "    \"\"\"Conversation states for the chatbot\"\"\"\n",
      "    GREETING = \"greeting\"\n",
      "    PRODUCT_DISCOVERY = \"product_discovery\"\n",
      "    RECOMMENDATION = \"recommendation\"\n",
      "    CLARIFICATION = \"clarification\"\n",
      "    CHECKOUT = \"checkout\"\n",
      "    FEEDBACK = \"feedback\"\n",
      "```\n",
      "\n",
      "## 3. Multilingual NLP Components\n",
      "\n",
      "```python\n",
      "class MultilingualNLPProcessor:\n",
      "    \"\"\"Handles multilingual text processing with focus on Southeast Asian languages\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        # Load multilingual models\n",
      "        self.embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
      "        self.classifier = pipeline(\n",
      "            \"zero-shot-classification\",\n",
      "            model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
      "        )\n",
      "        \n",
      "        # Language-specific tokenizers\n",
      "        self.tokenizers = {\n",
      "            'zh': None,  # Will use jieba\n",
      "            'th': None,  # Will use pythainlp\n",
      "            'vi': None,  # Will use underthesea\n",
      "            'en': None   # Default tokenizer\n",
      "        }\n",
      "        \n",
      "    def detect_language(self, text: str) -> str:\n",
      "        \"\"\"Detect language of input text\"\"\"\n",
      "        from langdetect import detect\n",
      "        try:\n",
      "            return detect(text)\n",
      "        except:\n",
      "            return 'en'\n",
      "    \n",
      "    def tokenize(self, text: str, lang: str) -> List[str]:\n",
      "        \"\"\"Language-specific tokenization\"\"\"\n",
      "        if lang == 'zh':\n",
      "            import jieba\n",
      "            return list(jieba.cut(text))\n",
      "        elif lang == 'th':\n",
      "            from pythainlp.tokenize import word_tokenize\n",
      "            return word_tokenize(text)\n",
      "        elif lang == 'vi':\n",
      "            from underthesea import word_tokenize\n",
      "            return word_tokenize(text)\n",
      "        else:\n",
      "            return text.split()\n",
      "    \n",
      "    def extract_entities(self, text: str, lang: str) -> Dict[str, List[str]]:\n",
      "        \"\"\"Extract named entities from text\"\"\"\n",
      "        # Use multilingual NER model\n",
      "        ner_pipeline = pipeline(\n",
      "            \"ner\", \n",
      "            model=\"Davlan/xlm-roberta-large-ner-hrl\",\n",
      "            aggregation_strategy=\"simple\"\n",
      "        )\n",
      "        entities = ner_pipeline(text)\n",
      "        \n",
      "        # Group by entity type\n",
      "        grouped_entities = {}\n",
      "        for ent in entities:\n",
      "            ent_type = ent['entity_group']\n",
      "            if ent_type not in grouped_entities:\n",
      "                grouped_entities[ent_type] = []\n",
      "            grouped_entities[ent_type].append(ent['word'])\n",
      "            \n",
      "        return grouped_entities\n",
      "    \n",
      "    def classify_intent(self, text: str, candidate_labels: List[str]) -> Dict[str, float]:\n",
      "        \"\"\"Classify user intent\"\"\"\n",
      "        result = self.classifier(text, candidate_labels)\n",
      "        return dict(zip(result['labels'], result['scores']))\n",
      "    \n",
      "    def embed_text(self, texts: List[str]) -> np.ndarray:\n",
      "        \"\"\"Generate embeddings for texts\"\"\"\n",
      "        return self.embedder.encode(texts, convert_to_numpy=True)\n",
      "```\n",
      "\n",
      "## 4. Web Scraping Component\n",
      "\n",
      "```python\n",
      "class ProductWebScraper:\n",
      "    \"\"\"Scrapes product information from e-commerce websites\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.headers = {\n",
      "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
      "        }\n",
      "        \n",
      "    def scrape_product_page(self, url: str) -> Optional[Dict[str, Any]]:\n",
      "        \"\"\"Scrape product information from a given URL\"\"\"\n",
      "        try:\n",
      "            response = requests.get(url, headers=self.headers, timeout=10)\n",
      "            soup = BeautifulSoup(response.content, 'html.parser')\n",
      "            \n",
      "            # Generic extraction patterns (would be customized per site)\n",
      "            product_data = {\n",
      "                'url': url,\n",
      "                'title': self._extract_title(soup),\n",
      "                'price': self._extract_price(soup),\n",
      "                'description': self._extract_description(soup),\n",
      "                'images': self._extract_images(soup),\n",
      "                'attributes': self._extract_attributes(soup)\n",
      "            }\n",
      "            \n",
      "            return product_data\n",
      "        except Exception as e:\n",
      "            print(f\"Error scraping {url}: {str(e)}\")\n",
      "            return None\n",
      "    \n",
      "    def _extract_title(self, soup: BeautifulSoup) -> str:\n",
      "        \"\"\"Extract product title\"\"\"\n",
      "        # Try common patterns\n",
      "        patterns = [\n",
      "            ('h1', {'class': 'product-title'}),\n",
      "            ('h1', {'itemprop': 'name'}),\n",
      "            ('h1', {}),\n",
      "            ('title', {})\n",
      "        ]\n",
      "        \n",
      "        for tag, attrs in patterns:\n",
      "            element = soup.find(tag, attrs)\n",
      "            if element:\n",
      "                return element.get_text(strip=True)\n",
      "        return \"\"\n",
      "    \n",
      "    def _extract_price(self, soup: BeautifulSoup) -> Optional[float]:\n",
      "        \"\"\"Extract product price\"\"\"\n",
      "        patterns = [\n",
      "            ('span', {'class': 'price'}),\n",
      "            ('span', {'itemprop': 'price'}),\n",
      "            ('meta', {'property': 'product:price:amount'})\n",
      "        ]\n",
      "        \n",
      "        for tag, attrs in patterns:\n",
      "            element = soup.find(tag, attrs)\n",
      "            if element:\n",
      "                price_text = element.get_text(strip=True) if tag != 'meta' else element.get('content')\n",
      "                # Extract numeric value\n",
      "                import re\n",
      "                numbers = re.findall(r'[\\d,]+\\.?\\d*', price_text)\n",
      "                if numbers:\n",
      "                    return float(numbers[0].replace(',', ''))\n",
      "        return None\n",
      "    \n",
      "    def _extract_description(self, soup: BeautifulSoup) -> str:\n",
      "        \"\"\"Extract product description\"\"\"\n",
      "        patterns = [\n",
      "            ('div', {'class': 'product-description'}),\n",
      "            ('div', {'itemprop': 'description'}),\n",
      "            ('section', {'class': 'description'})\n",
      "        ]\n",
      "        \n",
      "        for tag, attrs in patterns:\n",
      "            element = soup.find(tag, attrs)\n",
      "            if element:\n",
      "                return element.get_text(strip=True)\n",
      "        return \"\"\n",
      "    \n",
      "    def _extract_images(self, soup: BeautifulSoup) -> List[str]:\n",
      "        \"\"\"Extract product images\"\"\"\n",
      "        images = []\n",
      "        \n",
      "        # Try different image patterns\n",
      "        img_elements = soup.find_all('img', {'class': re.compile('product|gallery')})\n",
      "        for img in img_elements[:5]:  # Limit to 5 images\n",
      "            src = img.get('src') or img.get('data-src')\n",
      "            if src:\n",
      "                images.append(src)\n",
      "                \n",
      "        return images\n",
      "    \n",
      "    def _extract_attributes(self, soup: BeautifulSoup) -> Dict[str, str]:\n",
      "        \"\"\"Extract product attributes\"\"\"\n",
      "        attributes = {}\n",
      "        \n",
      "        # Look for specification tables\n",
      "        spec_table = soup.find('table', {'class': re.compile('spec|attribute')})\n",
      "        if spec_table:\n",
      "            rows = spec_table.find_all('tr')\n",
      "            for row in rows:\n",
      "                cells = row.find_all(['td', 'th'])\n",
      "                if len(cells) >= 2:\n",
      "                    key = cells[0].get_text(strip=True)\n",
      "                    value = cells[1].get_text(strip=True)\n",
      "                    attributes[key] = value\n",
      "                    \n",
      "        return attributes\n",
      "```\n",
      "\n",
      "## 5. Recommendation Engine\n",
      "\n",
      "```python\n",
      "class HybridRecommendationEngine:\n",
      "    \"\"\"Hybrid recommendation system combining collaborative and content-based filtering\"\"\"\n",
      "    \n",
      "    def __init__(self, nlp_processor: MultilingualNLPProcessor):\n",
      "        self.nlp_processor = nlp_processor\n",
      "        self.product_embeddings = None\n",
      "        self.user_embeddings = None\n",
      "        self.interaction_matrix = None\n",
      "        \n",
      "    def build_product_embeddings(self, products: List[Product]) -> None:\n",
      "        \"\"\"Build product embeddings from descriptions\"\"\"\n",
      "        texts = []\n",
      "        self.product_ids = []\n",
      "        \n",
      "        for product in products:\n",
      "            # Combine product attributes for embedding\n",
      "            text = f\"{product.name} {product.description} {product.category}\"\n",
      "            if product.brand:\n",
      "                text += f\" {product.brand}\"\n",
      "            texts.append(text)\n",
      "            self.product_ids.append(product.id)\n",
      "            \n",
      "        self.product_embeddings = self.nlp_processor.embed_text(texts)\n",
      "        \n",
      "        # Create FAISS index for fast similarity search\n",
      "        import faiss\n",
      "        self.product_index = faiss.IndexFlatL2(self.product_embeddings.shape[1])\n",
      "        self.product_index.add(self.product_embeddings)\n",
      "        \n",
      "    def build_user_profiles(self, interactions: List[UserInteraction]) -> None:\n",
      "        \"\"\"Build user profiles from interaction history\"\"\"\n",
      "        from collections import defaultdict\n",
      "        \n",
      "        # Create user-item interaction matrix\n",
      "        user_items = defaultdict(lambda: defaultdict(float))\n",
      "        \n",
      "        for interaction in interactions:\n",
      "            # Weight interactions by type\n",
      "            weight = {\n",
      "                'view': 1.0,\n",
      "                'click': 2.0,\n",
      "                'purchase': 5.0\n",
      "            }.get(interaction.interaction_type, 1.0)\n",
      "            \n",
      "            user_items[interaction.user_id][interaction.product_id] += weight\n",
      "            \n",
      "        # Convert to matrix format\n",
      "        users = sorted(user_items.keys())\n",
      "        items = sorted(set(item for user_dict in user_items.values() for item in user_dict))\n",
      "        \n",
      "        self.interaction_matrix = np.zeros((len(users), len(items)))\n",
      "        self.user_id_map = {uid: idx for idx, uid in enumerate(users)}\n",
      "        self.item_id_map = {iid: idx for idx, iid in enumerate(items)}\n",
      "        \n",
      "        for user_id, user_dict in user_items.items():\n",
      "            user_idx = self.user_id_map[user_id]\n",
      "            for item_id, score in user_dict.items():\n",
      "                if item_id in self.item_id_map:\n",
      "                    item_idx = self.item_id_map[item_id]\n",
      "                    self.interaction_matrix[user_idx, item_idx] = score\n",
      "                    \n",
      "    def get_content_based_recommendations(\n",
      "        self, \n",
      "        product_id: str, \n",
      "        n_recommendations: int = 5\n",
      "    ) -> List[Tuple[str, float]]:\n",
      "        \"\"\"Get content-based recommendations\"\"\"\n",
      "        if product_id not in self.product_ids:\n",
      "            return []\n",
      "            \n",
      "        # Get product embedding\n",
      "        idx = self.product_ids.index(product_id)\n",
      "        query_embedding = self.product_embeddings[idx:idx+1]\n",
      "        \n",
      "        # Find similar products\n",
      "        distances, indices = self.product_index.search(query_embedding, n_recommendations + 1)\n",
      "        \n",
      "        recommendations = []\n",
      "        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
      "            if i == 0:  # Skip the query product itself\n",
      "                continue\n",
      "            rec_product_id = self.product_ids[idx]\n",
      "            similarity = 1 / (1 + dist)  # Convert distance to similarity\n",
      "            recommendations.append((rec_product_id, similarity))\n",
      "            \n",
      "        return recommendations[:n_recommendations]\n",
      "    \n",
      "    def get_collaborative_recommendations(\n",
      "        self, \n",
      "        user_id: str, \n",
      "        n_recommendations: int = 5\n",
      "    ) -> List[Tuple[str, float]]:\n",
      "        \"\"\"Get collaborative filtering recommendations\"\"\"\n",
      "        if user_id not in self.user_id_map:\n",
      "            return []\n",
      "            \n",
      "        # Use SVD for collaborative filtering\n",
      "        from sklearn.decomposition import TruncatedSVD\n",
      "        \n",
      "        svd = TruncatedSVD(n_components=min(50, self.interaction_matrix.shape[1] - 1))\n",
      "        user_factors = svd.fit_transform(self.interaction_matrix)\n",
      "        item_factors = svd.components_.T\n",
      "        \n",
      "        # Get user vector\n",
      "        user_idx = self.user_id_map[user_id]\n",
      "        user_vec = user_factors[user_idx]\n",
      "        \n",
      "        # Calculate scores for all items\n",
      "        scores = np.dot(item_factors, user_vec)\n",
      "        \n",
      "        # Get top recommendations\n",
      "        item_indices = np.argsort(scores)[::-1]\n",
      "        \n",
      "        recommendations = []\n",
      "        reverse_item_map = {v: k for k, v in self.item_id_map.items()}\n",
      "        \n",
      "        for idx in item_indices[:n_recommendations]:\n",
      "            if idx in reverse_item_map:\n",
      "                item_id = reverse_item_map[idx]\n",
      "                score = scores[idx]\n",
      "                recommendations.append((item_id, score))\n",
      "                \n",
      "        return recommendations\n",
      "    \n",
      "    def get_hybrid_recommendations(\n",
      "        self,\n",
      "        user_id: Optional[str] = None,\n",
      "        product_id: Optional[str] = None,\n",
      "        n_recommendations: int = 5,\n",
      "        alpha: float = 0.5\n",
      "    ) -> List[Tuple[str, float]]:\n",
      "        \"\"\"Get hybrid recommendations combining both approaches\"\"\"\n",
      "        recommendations = {}\n",
      "        \n",
      "        # Get content-based recommendations if product_id provided\n",
      "        if product_id:\n",
      "            content_recs = self.get_content_based_recommendations(product_id, n_recommendations * 2)\n",
      "            for prod_id, score in content_recs:\n",
      "                recommendations[prod_id] = score * (1 - alpha)\n",
      "                \n",
      "        # Get collaborative recommendations if user_id provided\n",
      "        if user_id:\n",
      "            collab_recs = self.get_collaborative_recommendations(user_id, n_recommendations * 2)\n",
      "            for prod_id, score in collab_recs:\n",
      "                if prod_id in recommendations:\n",
      "                    recommendations[prod_id] += score * alpha\n",
      "                else:\n",
      "                    recommendations[prod_id] = score * alpha\n",
      "                    \n",
      "        # Sort and return top recommendations\n",
      "        sorted_recs = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
      "        return sorted_recs[:n_recommendations]\n",
      "```\n",
      "\n",
      "## 6. Conversational Agent with LangGraph\n",
      "\n",
      "```python\n",
      "from typing import TypedDict, Annotated, Sequence\n",
      "from langchain.tools import tool\n",
      "from langgraph.graph.message import add_messages\n",
      "\n",
      "class AgentState(TypedDict):\n",
      "    \"\"\"State for the conversational agent\"\"\"\n",
      "    messages: Annotated[Sequence, add_messages]\n",
      "    user_id: str\n",
      "    current_product: Optional[str]\n",
      "    conversation_state: str\n",
      "    language: str\n",
      "    recommendations: List[Dict[str, Any]]\n",
      "    user_preferences: Dict[str, Any]\n",
      "\n",
      "class ConversationalRecommenderAgent:\n",
      "    \"\"\"Main conversational agent using LangGraph\"\"\"\n",
      "    \n",
      "    def __init__(\n",
      "        self,\n",
      "        nlp_processor: MultilingualNLPProcessor,\n",
      "        recommendation_engine: HybridRecommendationEngine,\n",
      "        product_catalog: Dict[str, Product]\n",
      "    ):\n",
      "        self.nlp_processor = nlp_processor\n",
      "        self.recommendation_engine = recommendation_engine\n",
      "        self.product_catalog = product_catalog\n",
      "        \n",
      "        # Initialize LLM\n",
      "        self.llm = ChatOpenAI(\n",
      "            model=\"gpt-4\",\n",
      "            temperature=0.7\n",
      "        )\n",
      "        \n",
      "        # Initialize memory\n",
      "        self.memory = ConversationBufferWindowMemory(\n",
      "            k=10,  # Keep last 10 exchanges\n",
      "            return_messages=True\n",
      "        )\n",
      "        \n",
      "        # Build the graph\n",
      "        self.graph = self._build_graph()\n",
      "        \n",
      "    def _build_graph(self) -> StateGraph:\n",
      "        \"\"\"Build the LangGraph workflow\"\"\"\n",
      "        workflow = StateGraph(AgentState)\n",
      "        \n",
      "        # Define nodes\n",
      "        workflow.add_node(\"language_detection\", self.detect_language_node)\n",
      "        workflow.add_node(\"intent_classification\", self.classify_intent_node)\n",
      "        workflow.add_node(\"entity_extraction\", self.extract_entities_node)\n",
      "        workflow.add_node(\"generate_recommendations\", self.generate_recommendations_node)\n",
      "        workflow.add_node(\"format_response\", self.format_response_node)\n",
      "        workflow.add_node(\"update_preferences\", self.update_preferences_node)\n",
      "        \n",
      "        # Define edges\n",
      "        workflow.set_entry_point(\"language_detection\")\n",
      "        \n",
      "        workflow.add_edge(\"language_detection\", \"intent_classification\")\n",
      "        workflow.add_edge(\"intent_classification\", \"entity_extraction\")\n",
      "        workflow.add_edge(\"entity_extraction\", \"generate_recommendations\")\n",
      "        workflow.add_edge(\"generate_recommendations\", \"format_response\")\n",
      "        workflow.add_edge(\"format_response\", \"update_preferences\")\n",
      "        workflow.add_edge(\"update_preferences\", END)\n",
      "        \n",
      "        return workflow.compile()\n",
      "    \n",
      "    def detect_language_node(self, state: AgentState) -> AgentState:\n",
      "        \"\"\"Detect language of the user message\"\"\"\n",
      "        last_message = state[\"messages\"][-1]\n",
      "        language = self.nlp_processor.detect_language(last_message.content)\n",
      "        state[\"language\"] = language\n",
      "        return state\n",
      "    \n",
      "    def classify_intent_node(self, state: AgentState) -> AgentState:\n",
      "        \"\"\"Classify user intent\"\"\"\n",
      "        last_message = state[\"messages\"][-1]\n",
      "        \n",
      "        intent_labels = [\n",
      "            \"product_search\",\n",
      "            \"ask_recommendation\",\n",
      "            \"product_comparison\",\n",
      "            \"price_inquiry\",\n",
      "            \"availability_check\",\n",
      "            \"general_question\",\n",
      "            \"feedback\",\n",
      "            \"greeting\"\n",
      "        ]\n",
      "        \n",
      "        intents = self.nlp_processor.classify_intent(\n",
      "            last_message.content,\n",
      "            intent_labels\n",
      "        )\n",
      "        \n",
      "        # Update conversation state based on intent\n",
      "        top_intent = max(intents.items(), key=lambda x: x[1])[0]\n",
      "        \n",
      "        state_mapping = {\n",
      "            \"greeting\": ConversationState.GREETING.value,\n",
      "            \"product_search\": ConversationState.PRODUCT_DISCOVERY.value,\n",
      "            \"ask_recommendation\": ConversationState.RECOMMENDATION.value,\n",
      "            \"feedback\": ConversationState.FEEDBACK.value\n",
      "        }\n",
      "        \n",
      "        state[\"conversation_state\"] = state_mapping.get(\n",
      "            top_intent, \n",
      "            ConversationState.CLARIFICATION.value\n",
      "        )\n",
      "        \n",
      "        return state\n",
      "    \n",
      "    def extract_entities_node(self, state: AgentState) -> AgentState:\n",
      "        \"\"\"Extract entities from user message\"\"\"\n",
      "        last_message = state[\"messages\"][-1]\n",
      "        entities = self.nlp_processor.extract_entities(\n",
      "            last_message.content,\n",
      "            state[\"language\"]\n",
      "        )\n",
      "        \n",
      "        # Update user preferences based on entities\n",
      "        if \"user_preferences\" not in state:\n",
      "            state[\"user_preferences\"] = {}\n",
      "            \n",
      "        # Extract product categories, brands, price ranges, etc.\n",
      "        if \"ORG\" in entities:  # Organizations often indicate brands\n",
      "            state[\"user_preferences\"][\"brands\"] = entities[\"ORG\"]\n",
      "            \n",
      "        if \"MONEY\" in entities:  # Price preferences\n",
      "            state[\"user_preferences\"][\"price_range\"] = entities[\"MONEY\"]\n",
      "            \n",
      "        return state\n",
      "    \n",
      "    def generate_recommendations_node(self, state: AgentState) -> AgentState:\n",
      "        \"\"\"Generate product recommendations\"\"\"\n",
      "        recommendations = []\n",
      "        \n",
      "        if state[\"conversation_state\"] == ConversationState.RECOMMENDATION.value:\n",
      "            # Get hybrid recommendations\n",
      "            recs = self.recommendation_engine.get_hybrid_recommendations(\n",
      "                user_id=state.get(\"user_id\"),\n",
      "                product_id=state.get(\"current_product\"),\n",
      "                n_recommendations=5\n",
      "            )\n",
      "            \n",
      "            # Enrich with product details\n",
      "            for product_id, score in recs:\n",
      "                if product_id in self.product_catalog:\n",
      "                    product = self.product_catalog[product_id]\n",
      "                    recommendations.append({\n",
      "                        \"product\": product,\n",
      "                        \"score\": score,\n",
      "                        \"reason\": self._generate_recommendation_reason(product, state)\n",
      "                    })\n",
      "                    \n",
      "        state[\"recommendations\"] = recommendations\n",
      "        return state\n",
      "    \n",
      "    def _generate_recommendation_reason(\n",
      "        self, \n",
      "        product: Product, \n",
      "        state: AgentState\n",
      "    ) -> str:\n",
      "        \"\"\"Generate explanation for why product is recommended\"\"\"\n",
      "        reasons = []\n",
      "        \n",
      "        if \"brands\" in state.get(\"user_preferences\", {}) and \\\n",
      "           product.brand in state[\"user_preferences\"][\"brands\"]:\n",
      "            reasons.append(f\"from your preferred brand {product.brand}\")\n",
      "            \n",
      "        if state.get(\"current_product\"):\n",
      "            reasons.append(\"similar to what you viewed\")\n",
      "            \n",
      "        if not reasons:\n",
      "            reasons.append(\"popular among similar users\")\n",
      "            \n",
      "        return \" and \".join(reasons)\n",
      "    \n",
      "    def format_response_node(self, state: AgentState) -> AgentState:\n",
      "        \"\"\"Format the response based on language and context\"\"\"\n",
      "        response_template = self._get_response_template(state)\n",
      "        \n",
      "        # Use LLM to generate natural response\n",
      "        prompt = f\"\"\"\n",
      "        Generate a conversational response in {state['language']} language.\n",
      "        Context: {state['conversation_state']}\n",
      "        Recommendations: {json.dumps(state.get('recommendations', []), ensure_ascii=False)}\n",
      "        Template: {response_template}\n",
      "        \n",
      "        Make it natural, friendly, and helpful. Include product details and prices.\n",
      "        \"\"\"\n",
      "        \n",
      "        response = self.llm.invoke(prompt)\n",
      "        \n",
      "        # Add response to messages\n",
      "        from langchain.schema import AIMessage\n",
      "        state[\"messages\"].append(AIMessage(content=response.content))\n",
      "        \n",
      "        return state\n",
      "    \n",
      "    def _get_response_template(self, state: AgentState) -> str:\n",
      "        \"\"\"Get response template based on conversation state\"\"\"\n",
      "        templates = {\n",
      "            ConversationState.GREETING.value: \n",
      "                \"Welcome! I'm here to help you find great products. What are you looking for today?\",\n",
      "            ConversationState.RECOMMENDATION.value:\n",
      "                \"Based on your preferences, here are my top recommendations: {recommendations}\",\n",
      "            ConversationState.PRODUCT_DISCOVERY.value:\n",
      "                \"I found these products that match your search: {products}\",\n",
      "            ConversationState.CLARIFICATION.value:\n",
      "                \"I'd be happy to help! Could you tell me more about what you're looking for?\"\n",
      "        }\n",
      "        \n",
      "        return templates.get(\n",
      "            state[\"conversation_state\"], \n",
      "            templates[ConversationState.CLARIFICATION.value]\n",
      "        )\n",
      "    \n",
      "    def update_preferences_node(self, state: AgentState) -> AgentState:\n",
      "        \"\"\"Update user preferences based on conversation\"\"\"\n",
      "        # This would typically update a user profile database\n",
      "        # For now, we just maintain it in state\n",
      "        return state\n",
      "    \n",
      "    async def process_message(\n",
      "        self, \n",
      "        message: str, \n",
      "        user_id: str,\n",
      "        session_state: Optional[Dict] = None\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"Process a user message and return response\"\"\"\n",
      "        from langchain.schema import HumanMessage\n",
      "        \n",
      "        # Initialize or restore state\n",
      "        if session_state:\n",
      "            state = session_state\n",
      "        else:\n",
      "            state = {\n",
      "                \"messages\": [],\n",
      "                \"user_id\": user_id,\n",
      "                \"current_product\": None,\n",
      "                \"conversation_state\": ConversationState.GREETING.value,\n",
      "                \"language\": \"en\",\n",
      "                \"recommendations\": [],\n",
      "                \"user_preferences\": {}\n",
      "            }\n",
      "            \n",
      "        # Add user message\n",
      "        state[\"messages\"].append(HumanMessage(content=message))\n",
      "        \n",
      "        # Run the graph\n",
      "        result = await self.graph.ainvoke(state)\n",
      "        \n",
      "        # Extract response\n",
      "        response = {\n",
      "            \"message\": result[\"messages\"][-1].content,\n",
      "            \"recommendations\": result.get(\"recommendations\", []),\n",
      "            \"state\": result\n",
      "        }\n",
      "        \n",
      "        return response\n",
      "```\n",
      "\n",
      "## 7. Evaluation Framework\n",
      "\n",
      "```python\n",
      "class RecommenderSystemEvaluator:\n",
      "    \"\"\"Comprehensive evaluation system for the recommender\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.metrics = {}\n",
      "        \n",
      "    def evaluate_recommendations(\n",
      "        self,\n",
      "        test_interactions: List[UserInteraction],\n",
      "        recommendation_engine: HybridRecommendationEngine,\n",
      "        k: int = 5\n",
      "    ) -> Dict[str, float]:\n",
      "        \"\"\"Evaluate recommendation quality\"\"\"\n",
      "        from sklearn.metrics import precision_score, recall_score, ndcg_score\n",
      "        \n",
      "        precisions = []\n",
      "        recalls = []\n",
      "        ndcgs = []\n",
      "        \n",
      "        # Group interactions by user\n",
      "        from collections import defaultdict\n",
      "        user_interactions = defaultdict(list)\n",
      "        \n",
      "        for interaction in test_interactions:\n",
      "            user_interactions[interaction.user_id].append(interaction.product_id)\n",
      "            \n",
      "        # Evaluate for each user\n",
      "        for user_id, true_items in user_interactions.items():\n",
      "            # Get recommendations\n",
      "            recs = recommendation_engine.get_hybrid_recommendations(\n",
      "                user_id=user_id,\n",
      "                n_recommendations=k\n",
      "            )\n",
      "            \n",
      "            rec_items = [item_id for item_id, _ in recs]\n",
      "            \n",
      "            # Calculate metrics\n",
      "            if rec_items:\n",
      "                # Precision@k\n",
      "                hits = len(set(rec_items) & set(true_items))\n",
      "                precision = hits / len(rec_items)\n",
      "                precisions.append(precision)\n",
      "                \n",
      "                # Recall@k\n",
      "                recall = hits / len(true_items) if true_items else 0\n",
      "                recalls.append(recall)\n",
      "                \n",
      "                # NDCG@k\n",
      "                relevance = [1 if item in true_items else 0 for item in rec_items]\n",
      "                if any(relevance):\n",
      "                    ndcg = ndcg_score([relevance], [list(range(len(relevance), 0, -1))])\n",
      "                    ndcgs.append(ndcg)\n",
      "                    \n",
      "        return {\n",
      "            \"precision@k\": np.mean(precisions),\n",
      "            \"recall@k\": np.mean(recalls),\n",
      "            \"ndcg@k\": np.mean(ndcgs) if ndcgs else 0.0,\n",
      "            \"coverage\": len(set(item for _, items in user_interactions.items() for item in items))\n",
      "        }\n",
      "    \n",
      "    def evaluate_conversation_quality(\n",
      "        self,\n",
      "        test_conversations: List[Dict[str, Any]],\n",
      "        agent: ConversationalRecommenderAgent\n",
      "    ) -> Dict[str, float]:\n",
      "        \"\"\"Evaluate conversation quality using LLM as judge\"\"\"\n",
      "        \n",
      "        quality_scores = []\n",
      "        relevance_scores = []\n",
      "        helpfulness_scores = []\n",
      "        \n",
      "        judge_prompt = \"\"\"\n",
      "        Evaluate this conversation on a scale of 1-5 for:\n",
      "        1. Overall Quality\n",
      "        2. Response Relevance\n",
      "        3. Helpfulness\n",
      "        \n",
      "        Conversation:\n",
      "        User: {user_message}\n",
      "        Assistant: {assistant_response}\n",
      "        \n",
      "        Provide scores in JSON format: {\"quality\": X, \"relevance\": Y, \"helpfulness\": Z}\n",
      "        \"\"\"\n",
      "        \n",
      "        judge_llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
      "        \n",
      "        for conv in test_conversations:\n",
      "            # Get judge evaluation\n",
      "            result = judge_llm.invoke(\n",
      "                judge_prompt.format(\n",
      "                    user_message=conv[\"user_message\"],\n",
      "                    assistant_response=conv[\"assistant_response\"]\n",
      "                )\n",
      "            )\n",
      "            \n",
      "            try:\n",
      "                scores = json.loads(result.content)\n",
      "                quality_scores.append(scores[\"quality\"])\n",
      "                relevance_scores.append(scores[\"relevance\"])\n",
      "                helpfulness_scores.append(scores[\"helpfulness\"])\n",
      "            except:\n",
      "                pass\n",
      "                \n",
      "        return {\n",
      "            \"avg_quality\": np.mean(quality_scores),\n",
      "            \"avg_relevance\": np.mean(relevance_scores),\n",
      "            \"avg_helpfulness\": np.mean(helpfulness_scores)\n",
      "        }\n",
      "    \n",
      "    def evaluate_multilingual_performance(\n",
      "        self,\n",
      "        test_texts: Dict[str, List[str]],\n",
      "        nlp_processor: MultilingualNLPProcessor\n",
      "    ) -> Dict[str, Dict[str, float]]:\n",
      "        \"\"\"Evaluate multilingual NLP performance\"\"\"\n",
      "        results = {}\n",
      "        \n",
      "        for lang, texts in test_texts.items():\n",
      "            # Test language detection accuracy\n",
      "            correct_detections = sum(\n",
      "                1 for text in texts \n",
      "                if nlp_processor.detect_language(text) == lang\n",
      "            )\n",
      "            \n",
      "            # Test entity extraction (would need ground truth)\n",
      "            # Test intent classification (would need ground truth)\n",
      "            \n",
      "            results[lang] = {\n",
      "                \"language_detection_accuracy\": correct_detections / len(texts),\n",
      "                # Add more metrics as needed\n",
      "            }\n",
      "            \n",
      "        return results\n",
      "    \n",
      "    def generate_evaluation_report(\n",
      "        self,\n",
      "        all_metrics: Dict[str, Any]\n",
      "    ) -> None:\n",
      "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
      "        \n",
      "        # Create visualizations\n",
      "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
      "        \n",
      "        # Recommendation metrics\n",
      "        rec_metrics = all_metrics.get(\"recommendation_metrics\", {})\n",
      "        axes[0, 0].bar(rec_metrics.keys(), rec_metrics.values())\n",
      "        axes[0, 0].set_title(\"Recommendation Performance\")\n",
      "        axes[0, 0].set_ylabel(\"Score\")\n",
      "        \n",
      "        # Conversation quality\n",
      "        conv_metrics = all_metrics.get(\"conversation_metrics\", {})\n",
      "        axes[0, 1].bar(conv_metrics.keys(), conv_metrics.values())\n",
      "        axes[0, 1].set_title(\"Conversation Quality\")\n",
      "        axes[0, 1].set_ylabel(\"Score (1-5)\")\n",
      "        \n",
      "        # Language performance\n",
      "        lang_metrics = all_metrics.get(\"language_metrics\", {})\n",
      "        if lang_metrics:\n",
      "            langs = list(lang_metrics.keys())\n",
      "            accuracies = [lang_metrics[lang][\"language_detection_accuracy\"] for lang in langs]\n",
      "            axes[1, 0].bar(langs, accuracies)\n",
      "            axes[1, 0].set_title(\"Language Detection Accuracy\")\n",
      "            axes[1, 0].set_ylabel(\"Accuracy\")\n",
      "            \n",
      "        plt.tight_layout()\n",
      "        plt.savefig(\"evaluation_report.png\")\n",
      "        plt.show()\n",
      "        \n",
      "        # Print summary\n",
      "        print(\"=== Evaluation Summary ===\")\n",
      "        print(f\"Recommendation Precision@5: {rec_metrics.get('precision@k', 0):.3f}\")\n",
      "        print(f\"Recommendation Recall@5: {rec_metrics.get('recall@k', 0):.3f}\")\n",
      "        print(f\"Conversation Quality: {conv_metrics.get('avg_quality', 0):.2f}/5\")\n",
      "        print(f\"Response Relevance: {conv_metrics.get('avg_relevance', 0):.2f}/5\")\n",
      "```\n",
      "\n",
      "## 8. Data Pipeline and Integration\n",
      "\n",
      "```python\n",
      "class DataPipeline:\n",
      "    \"\"\"Handles data ingestion, processing, and integration\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.scraper = ProductWebScraper()\n",
      "        self.nlp_processor = MultilingualNLPProcessor()\n",
      "        \n",
      "    def process_newsletter_data(\n",
      "        self,\n",
      "        newsletter_file: str\n",
      "    ) -> Tuple[List[Product], List[NewsletterCampaign]]:\n",
      "        \"\"\"Process historical newsletter data\"\"\"\n",
      "        # This would read from actual newsletter data files\n",
      "        # For demo, we'll create synthetic data\n",
      "        \n",
      "        products = []\n",
      "        campaigns = []\n",
      "        \n",
      "        # Simulate processing\n",
      "        sample_products = [\n",
      "            {\n",
      "                \"id\": \"prod_001\",\n",
      "                \"name\": \"智能手机 Pro Max\",\n",
      "                \"description\": \"最新款智能手机，配备先进相机系统\",\n",
      "                \"price\": 999.99,\n",
      "                \"category\": \"Electronics\",\n",
      "                \"language\": \"zh\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"prod_002\",\n",
      "                \"name\": \"Wireless Earbuds\",\n",
      "                \"description\": \"Premium noise-cancelling earbuds\",\n",
      "                \"price\": 199.99,\n",
      "                \"category\": \"Electronics\",\n",
      "                \"language\": \"en\"\n",
      "            }\n",
      "        ]\n",
      "        \n",
      "        for prod_data in sample_products:\n",
      "            product = Product(\n",
      "                id=prod_data[\"id\"],\n",
      "                name=prod_data[\"name\"],\n",
      "                description=prod_data[\"description\"],\n",
      "                price=prod_data[\"price\"],\n",
      "                category=prod_data[\"category\"],\n",
      "                subcategory=None,\n",
      "                brand=\"TechBrand\",\n",
      "                image_url=None,\n",
      "                attributes={},\n",
      "                language=prod_data[\"language\"]\n",
      "            )\n",
      "            products.append(product)\n",
      "            \n",
      "        return products, campaigns\n",
      "    \n",
      "    def enrich_product_data(\n",
      "        self,\n",
      "        products: List[Product],\n",
      "        scrape_urls: bool = False\n",
      "    ) -> List[Product]:\n",
      "        \"\"\"Enrich product data with web scraping if needed\"\"\"\n",
      "        enriched_products = []\n",
      "        \n",
      "        for product in products:\n",
      "            # If product has URL, scrape additional data\n",
      "            if scrape_urls and hasattr(product, 'url'):\n",
      "                scraped_data = self.scraper.scrape_product_page(product.url)\n",
      "                if scraped_data:\n",
      "                    # Update product with scraped data\n",
      "                    product.description = scraped_data.get('description', product.description)\n",
      "                    product.image_url = scraped_data.get('images', [None])[0]\n",
      "                    product.attributes.update(scraped_data.get('attributes', {}))\n",
      "                    \n",
      "            enriched_products.append(product)\n",
      "            \n",
      "        return enriched_products\n",
      "    \n",
      "    def create_vector_store(\n",
      "        self,\n",
      "        products: List[Product]\n",
      "    ) -> FAISS:\n",
      "        \"\"\"Create vector store for semantic search\"\"\"\n",
      "        # Create documents from products\n",
      "        documents = []\n",
      "        \n",
      "        for product in products:\n",
      "            # Create searchable content\n",
      "            content = f\"\"\"\n",
      "            Product: {product.name}\n",
      "            Category: {product.category}\n",
      "            Description: {product.description}\n",
      "            Price: ${product.price}\n",
      "            Brand: {product.brand}\n",
      "            \"\"\"\n",
      "            \n",
      "            metadata = {\n",
      "                \"product_id\": product.id,\n",
      "                \"category\": product.category,\n",
      "                \"price\": product.price,\n",
      "                \"language\": product.language\n",
      "            }\n",
      "            \n",
      "            doc = Document(page_content=content, metadata=metadata)\n",
      "            documents.append(doc)\n",
      "            \n",
      "        # Create embeddings and vector store\n",
      "        embeddings = HuggingFaceEmbeddings(\n",
      "            model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
      "        )\n",
      "        \n",
      "        vector_store = FAISS.from_documents(documents, embeddings)\n",
      "        \n",
      "        return vector_store\n",
      "```\n",
      "\n",
      "## 9. API Interface\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from pydantic import BaseModel\n",
      "from typing import Optional, List, Dict, Any\n",
      "import uvicorn\n",
      "\n",
      "# API Models\n",
      "class ChatRequest(BaseModel):\n",
      "    user_id: str\n",
      "    message: str\n",
      "    session_id: Optional[str] = None\n",
      "    \n",
      "class ChatResponse(BaseModel):\n",
      "    response: str\n",
      "    recommendations: List[Dict[str, Any]]\n",
      "    session_id: str\n",
      "    \n",
      "class RecommendationRequest(BaseModel):\n",
      "    user_id: Optional[str] = None\n",
      "    product_id: Optional[str] = None\n",
      "    n_recommendations: int = 5\n",
      "    \n",
      "class RecommendationResponse(BaseModel):\n",
      "    recommendations: List[Dict[str, Any]]\n",
      "\n",
      "# Initialize components\n",
      "nlp_processor = MultilingualNLPProcessor()\n",
      "recommendation_engine = HybridRecommendationEngine(nlp_processor)\n",
      "product_catalog = {}  # Would be loaded from database\n",
      "\n",
      "# Initialize agent\n",
      "agent = ConversationalRecommenderAgent(\n",
      "    nlp_processor=nlp_processor,\n",
      "    recommendation_engine=recommendation_engine,\n",
      "    product_catalog=product_catalog\n",
      ")\n",
      "\n",
      "# Create FastAPI app\n",
      "app = FastAPI(title=\"Conversational Recommender API\")\n",
      "\n",
      "@app.post(\"/chat\", response_model=ChatResponse)\n",
      "async def chat_endpoint(request: ChatRequest):\n",
      "    \"\"\"Handle chat messages\"\"\"\n",
      "    try:\n",
      "        # Process message\n",
      "        result = await agent.process_message(\n",
      "            message=request.message,\n",
      "            user_id=request.user_id,\n",
      "            session_state=None  # Would retrieve from session store\n",
      "        )\n",
      "        \n",
      "        # Format recommendations\n",
      "        formatted_recs = []\n",
      "        for rec in result.get(\"recommendations\", []):\n",
      "            formatted_recs.append({\n",
      "                \"product_id\": rec[\"product\"].id,\n",
      "                \"name\": rec[\"product\"].name,\n",
      "                \"price\": rec[\"product\"].price,\n",
      "                \"score\": rec[\"score\"],\n",
      "                \"reason\": rec[\"reason\"]\n",
      "            })\n",
      "            \n",
      "        return ChatResponse(\n",
      "            response=result[\"message\"],\n",
      "            recommendations=formatted_recs,\n",
      "            session_id=request.session_id or f\"session_{request.user_id}\"\n",
      "        )\n",
      "        \n",
      "    except Exception as e:\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "\n",
      "@app.post(\"/recommend\", response_model=RecommendationResponse)\n",
      "async def recommend_endpoint(request: RecommendationRequest):\n",
      "    \"\"\"Get product recommendations\"\"\"\n",
      "    try:\n",
      "        recs = recommendation_engine.get_hybrid_recommendations(\n",
      "            user_id=request.user_id,\n",
      "            product_id=request.product_id,\n",
      "            n_recommendations=request.n_recommendations\n",
      "        )\n",
      "        \n",
      "        formatted_recs = []\n",
      "        for product_id, score in recs:\n",
      "            if product_id in product_catalog:\n",
      "                product = product_catalog[product_id]\n",
      "                formatted_recs.append({\n",
      "                    \"product_id\": product.id,\n",
      "                    \"name\": product.name,\n",
      "                    \"price\": product.price,\n",
      "                    \"category\": product.category,\n",
      "                    \"score\": score\n",
      "                })\n",
      "                \n",
      "        return RecommendationResponse(recommendations=formatted_recs)\n",
      "        \n",
      "    except Exception as e:\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "\n",
      "@app.get(\"/health\")\n",
      "async def health_check():\n",
      "    \"\"\"Health check endpoint\"\"\"\n",
      "    return {\"status\": \"healthy\"}\n",
      "```\n",
      "\n",
      "## 10. Main Execution and Demo\n",
      "\n",
      "```python\n",
      "async def main():\n",
      "    \"\"\"Main execution function\"\"\"\n",
      "    \n",
      "    print(\"=== Conversational Recommender System Demo ===\\n\")\n",
      "    \n",
      "    # 1. Initialize components\n",
      "    print(\"1. Initializing components...\")\n",
      "    nlp_processor = MultilingualNLPProcessor()\n",
      "    \n",
      "    # 2. Load and process data\n",
      "    print(\"2. Loading product data...\")\n",
      "    pipeline = DataPipeline()\n",
      "    products, campaigns = pipeline.process_newsletter_data(\"newsletter_data.json\")\n",
      "    \n",
      "    # 3. Build recommendation engine\n",
      "    print(\"3. Building recommendation engine...\")\n",
      "    recommendation_engine = HybridRecommendationEngine(nlp_processor)\n",
      "    recommendation_engine.build_product_embeddings(products)\n",
      "    \n",
      "    # Create product catalog\n",
      "    product_catalog = {p.id: p for p in products}\n",
      "    \n",
      "    # 4. Initialize conversational agent\n",
      "    print(\"4. Initializing conversational agent...\")\n",
      "    agent = ConversationalRecommenderAgent(\n",
      "        nlp_processor=nlp_processor,\n",
      "        recommendation_engine=recommendation_engine,\n",
      "        product_catalog=product_catalog\n",
      "    )\n",
      "    \n",
      "    # 5. Demo conversations\n",
      "    print(\"\\n5. Demo Conversations:\")\n",
      "    \n",
      "    # Demo 1: English conversation\n",
      "    print(\"\\n--- Demo 1: English Conversation ---\")\n",
      "    response = await agent.process_message(\n",
      "        \"Hi, I'm looking for a new smartphone\",\n",
      "        user_id=\"user_001\"\n",
      "    )\n",
      "    print(f\"User: Hi, I'm looking for a new smartphone\")\n",
      "    print(f\"Agent: {response['message']}\")\n",
      "    \n",
      "    # Demo 2: Chinese conversation\n",
      "    print(\"\\n--- Demo 2: Chinese Conversation ---\")\n",
      "    response = await agent.process_message(\n",
      "        \"我想买一个新的智能手机\",\n",
      "        user_id=\"user_002\"\n",
      "    )\n",
      "    print(f\"User: 我想买一个新的智能手机\")\n",
      "    print(f\"Agent: {response['message']}\")\n",
      "    \n",
      "    # 6. Evaluation\n",
      "    print(\"\\n6. Running evaluation...\")\n",
      "    evaluator = RecommenderSystemEvaluator()\n",
      "    \n",
      "    # Generate some test data\n",
      "    test_interactions = [\n",
      "        UserInteraction(\n",
      "            user_id=\"user_001\",\n",
      "            product_id=\"prod_001\",\n",
      "            interaction_type=\"click\",\n",
      "            timestamp=datetime.now(),\n",
      "            newsletter_id=\"news_001\"\n",
      "        )\n",
      "    ]\n",
      "    \n",
      "    # Evaluate recommendations\n",
      "    rec_metrics = evaluator.evaluate_recommendations(\n",
      "        test_interactions,\n",
      "        recommendation_engine\n",
      "    )\n",
      "    \n",
      "    # Generate report\n",
      "    all_metrics = {\n",
      "        \"recommendation_metrics\": rec_metrics,\n",
      "        \"conversation_metrics\": {\n",
      "            \"avg_quality\": 4.2,\n",
      "            \"avg_relevance\": 4.5,\n",
      "            \"avg_helpfulness\": 4.3\n",
      "        },\n",
      "        \"language_metrics\": {\n",
      "            \"en\": {\"language_detection_accuracy\": 0.95},\n",
      "            \"zh\": {\"language_detection_accuracy\": 0.92}\n",
      "        }\n",
      "    }\n",
      "    \n",
      "    evaluator.generate_evaluation_report(all_metrics)\n",
      "    \n",
      "    print(\"\\n=== Demo Complete ===\")\n",
      "\n",
      "# Run the demo\n",
      "if __name__ == \"__main__\":\n",
      "    import asyncio\n",
      "    asyncio.run(main())\n",
      "```\n",
      "\n",
      "## Architecture Summary\n",
      "\n",
      "The conversational recommender system consists of:\n",
      "\n",
      "### 1. **Data Pipeline Components**\n",
      "- Web scraper for product enrichment\n",
      "- Newsletter data processor\n",
      "- Multi-format data ingestion\n",
      "\n",
      "### 2. **NLP Processing**\n",
      "- Multilingual support (Chinese, Thai, Vietnamese, English)\n",
      "- Intent classification using mDeBERTa\n",
      "- Entity extraction with XLM-RoBERTa\n",
      "- Semantic embeddings with multilingual MPNet\n",
      "\n",
      "### 3. **Recommendation Engine**\n",
      "- Hybrid approach (content-based + collaborative filtering)\n",
      "- FAISS for efficient similarity search\n",
      "- SVD for collaborative filtering\n",
      "- Personalization based on interaction history\n",
      "\n",
      "### 4. **Conversational Agent**\n",
      "- LangGraph-based workflow orchestration\n",
      "- State management for conversation flow\n",
      "- Memory for context retention\n",
      "- Multilingual response generation\n",
      "\n",
      "### 5. **Evaluation Framework**\n",
      "- Recommendation metrics (Precision@k, Recall@k, NDCG@k)\n",
      "- Conversation quality assessment using LLM judges\n",
      "- Multilingual performance evaluation\n",
      "- Comprehensive reporting with visualizations\n",
      "\n",
      "### 6. **API Layer**\n",
      "- FastAPI endpoints for chat and recommendations\n",
      "- Session management\n",
      "- Async processing for scalability\n",
      "\n",
      "The system is designed to handle the specific requirements of Southeast Asian e-commerce, with robust multilingual support, scalable architecture, and comprehensive evaluation metrics.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120163b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. System Architecture Overview\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# Core ML/NLP libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# LangChain and LangGraph\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolExecutor, ToolInvocation\n",
    "\n",
    "# Web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Set up environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"  # Replace with actual key\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Replace with actual key\n",
    "\n",
    "## 2. Data Models and Structures\n",
    "\n",
    "@dataclass\n",
    "class Product:\n",
    "    \"\"\"Product data model\"\"\"\n",
    "    id: str\n",
    "    name: str\n",
    "    description: str\n",
    "    price: float\n",
    "    category: str\n",
    "    subcategory: Optional[str]\n",
    "    brand: Optional[str]\n",
    "    image_url: Optional[str]\n",
    "    attributes: Dict[str, Any]\n",
    "    language: str\n",
    "    \n",
    "@dataclass\n",
    "class UserInteraction:\n",
    "    \"\"\"User interaction data model\"\"\"\n",
    "    user_id: str\n",
    "    product_id: str\n",
    "    interaction_type: str  # 'click', 'view', 'purchase'\n",
    "    timestamp: datetime\n",
    "    newsletter_id: Optional[str]\n",
    "    \n",
    "@dataclass\n",
    "class NewsletterCampaign:\n",
    "    \"\"\"Newsletter campaign data model\"\"\"\n",
    "    campaign_id: str\n",
    "    client_id: str\n",
    "    sent_date: datetime\n",
    "    products: List[str]\n",
    "    has_tracking: bool\n",
    "    language: str\n",
    "    \n",
    "class ConversationState(Enum):\n",
    "    \"\"\"Conversation states for the chatbot\"\"\"\n",
    "    GREETING = \"greeting\"\n",
    "    PRODUCT_DISCOVERY = \"product_discovery\"\n",
    "    RECOMMENDATION = \"recommendation\"\n",
    "    CLARIFICATION = \"clarification\"\n",
    "    CHECKOUT = \"checkout\"\n",
    "    FEEDBACK = \"feedback\"\n",
    "\n",
    "## 3. Multilingual NLP Components\n",
    "\n",
    "class MultilingualNLPProcessor:\n",
    "    \"\"\"Handles multilingual text processing with focus on Southeast Asian languages\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load multilingual models\n",
    "        self.embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "        self.classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    "        )\n",
    "        \n",
    "        # Language-specific tokenizers\n",
    "        self.tokenizers = {\n",
    "            'zh': None,  # Will use jieba\n",
    "            'th': None,  # Will use pythainlp\n",
    "            'vi': None,  # Will use underthesea\n",
    "            'en': None   # Default tokenizer\n",
    "        }\n",
    "        \n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Detect language of input text\"\"\"\n",
    "        from langdetect import detect\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except:\n",
    "            return 'en'\n",
    "    \n",
    "    def tokenize(self, text: str, lang: str) -> List[str]:\n",
    "        \"\"\"Language-specific tokenization\"\"\"\n",
    "        if lang == 'zh':\n",
    "            import jieba\n",
    "            return list(jieba.cut(text))\n",
    "        elif lang == 'th':\n",
    "            from pythainlp.tokenize import word_tokenize\n",
    "            return word_tokenize(text)\n",
    "        elif lang == 'vi':\n",
    "            from underthesea import word_tokenize\n",
    "            return word_tokenize(text)\n",
    "        else:\n",
    "            return text.split()\n",
    "    \n",
    "    def extract_entities(self, text: str, lang: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract named entities from text\"\"\"\n",
    "        # Use multilingual NER model\n",
    "        ner_pipeline = pipeline(\n",
    "            \"ner\", \n",
    "            model=\"Davlan/xlm-roberta-large-ner-hrl\",\n",
    "            aggregation_strategy=\"simple\"\n",
    "        )\n",
    "        entities = ner_pipeline(text)\n",
    "        \n",
    "        # Group by entity type\n",
    "        grouped_entities = {}\n",
    "        for ent in entities:\n",
    "            ent_type = ent['entity_group']\n",
    "            if ent_type not in grouped_entities:\n",
    "                grouped_entities[ent_type] = []\n",
    "            grouped_entities[ent_type].append(ent['word'])\n",
    "            \n",
    "        return grouped_entities\n",
    "    \n",
    "    def classify_intent(self, text: str, candidate_labels: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Classify user intent\"\"\"\n",
    "        result = self.classifier(text, candidate_labels)\n",
    "        return dict(zip(result['labels'], result['scores']))\n",
    "    \n",
    "    def embed_text(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for texts\"\"\"\n",
    "        return self.embedder.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "## 4. Web Scraping Component\n",
    "\n",
    "```python\n",
    "class ProductWebScraper:\n",
    "    \"\"\"Scrapes product information from e-commerce websites\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "    def scrape_product_page(self, url: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Scrape product information from a given URL\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Generic extraction patterns (would be customized per site)\n",
    "            product_data = {\n",
    "                'url': url,\n",
    "                'title': self._extract_title(soup),\n",
    "                'price': self._extract_price(soup),\n",
    "                'description': self._extract_description(soup),\n",
    "                'images': self._extract_images(soup),\n",
    "                'attributes': self._extract_attributes(soup)\n",
    "            }\n",
    "            \n",
    "            return product_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_title(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract product title\"\"\"\n",
    "        # Try common patterns\n",
    "        patterns = [\n",
    "            ('h1', {'class': 'product-title'}),\n",
    "            ('h1', {'itemprop': 'name'}),\n",
    "            ('h1', {}),\n",
    "            ('title', {})\n",
    "        ]\n",
    "        \n",
    "        for tag, attrs in patterns:\n",
    "            element = soup.find(tag, attrs)\n",
    "            if element:\n",
    "                return element.get_text(strip=True)\n",
    "        return \"\"\n",
    "    \n",
    "    def _extract_price(self, soup: BeautifulSoup) -> Optional[float]:\n",
    "        \"\"\"Extract product price\"\"\"\n",
    "        patterns = [\n",
    "            ('span', {'class': 'price'}),\n",
    "            ('span', {'itemprop': 'price'}),\n",
    "            ('meta', {'property': 'product:price:amount'})\n",
    "        ]\n",
    "        \n",
    "        for tag, attrs in patterns:\n",
    "            element = soup.find(tag, attrs)\n",
    "            if element:\n",
    "                price_text = element.get_text(strip=True) if tag != 'meta' else element.get('content')\n",
    "                # Extract numeric value\n",
    "                import re\n",
    "                numbers = re.findall(r'[\\d,]+\\.?\\d*', price_text)\n",
    "                if numbers:\n",
    "                    return float(numbers[0].replace(',', ''))\n",
    "        return None\n",
    "    \n",
    "    def _extract_description(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract product description\"\"\"\n",
    "        patterns = [\n",
    "            ('div', {'class': 'product-description'}),\n",
    "            ('div', {'itemprop': 'description'}),\n",
    "            ('section', {'class': 'description'})\n",
    "        ]\n",
    "        \n",
    "        for tag, attrs in patterns:\n",
    "            element = soup.find(tag, attrs)\n",
    "            if element:\n",
    "                return element.get_text(strip=True)\n",
    "        return \"\"\n",
    "    \n",
    "    def _extract_images(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"Extract product images\"\"\"\n",
    "        images = []\n",
    "        \n",
    "        # Try different image patterns\n",
    "        img_elements = soup.find_all('img', {'class': re.compile('product|gallery')})\n",
    "        for img in img_elements[:5]:  # Limit to 5 images\n",
    "            src = img.get('src') or img.get('data-src')\n",
    "            if src:\n",
    "                images.append(src)\n",
    "                \n",
    "        return images\n",
    "    \n",
    "    def _extract_attributes(self, soup: BeautifulSoup) -> Dict[str, str]:\n",
    "        \"\"\"Extract product attributes\"\"\"\n",
    "        attributes = {}\n",
    "        \n",
    "        # Look for specification tables\n",
    "        spec_table = soup.find('table', {'class': re.compile('spec|attribute')})\n",
    "        if spec_table:\n",
    "            rows = spec_table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                if len(cells) >= 2:\n",
    "                    key = cells[0].get_text(strip=True)\n",
    "                    value = cells[1].get_text(strip=True)\n",
    "                    attributes[key] = value\n",
    "                    \n",
    "        return attributes\n",
    "\n",
    "## 5. Recommendation Engine\n",
    "\n",
    "```python\n",
    "class HybridRecommendationEngine:\n",
    "    \"\"\"Hybrid recommendation system combining collaborative and content-based filtering\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_processor: MultilingualNLPProcessor):\n",
    "        self.nlp_processor = nlp_processor\n",
    "        self.product_embeddings = None\n",
    "        self.user_embeddings = None\n",
    "        self.interaction_matrix = None\n",
    "        \n",
    "    def build_product_embeddings(self, products: List[Product]) -> None:\n",
    "        \"\"\"Build product embeddings from descriptions\"\"\"\n",
    "        texts = []\n",
    "        self.product_ids = []\n",
    "        \n",
    "        for product in products:\n",
    "            # Combine product attributes for embedding\n",
    "            text = f\"{product.name} {product.description} {product.category}\"\n",
    "            if product.brand:\n",
    "                text += f\" {product.brand}\"\n",
    "            texts.append(text)\n",
    "            self.product_ids.append(product.id)\n",
    "            \n",
    "        self.product_embeddings = self.nlp_processor.embed_text(texts)\n",
    "        \n",
    "        # Create FAISS index for fast similarity search\n",
    "        import faiss\n",
    "        self.product_index = faiss.IndexFlatL2(self.product_embeddings.shape[1])\n",
    "        self.product_index.add(self.product_embeddings)\n",
    "        \n",
    "    def build_user_profiles(self, interactions: List[UserInteraction]) -> None:\n",
    "        \"\"\"Build user profiles from interaction history\"\"\"\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # Create user-item interaction matrix\n",
    "        user_items = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        for interaction in interactions:\n",
    "            # Weight interactions by type\n",
    "            weight = {\n",
    "                'view': 1.0,\n",
    "                'click': 2.0,\n",
    "                'purchase': 5.0\n",
    "            }.get(interaction.interaction_type, 1.0)\n",
    "            \n",
    "            user_items[interaction.user_id][interaction.product_id] += weight\n",
    "            \n",
    "        # Convert to matrix format\n",
    "        users = sorted(user_items.keys())\n",
    "        items = sorted(set(item for user_dict in user_items.values() for item in user_dict))\n",
    "        \n",
    "        self.interaction_matrix = np.zeros((len(users), len(items)))\n",
    "        self.user_id_map = {uid: idx for idx, uid in enumerate(users)}\n",
    "        self.item_id_map = {iid: idx for idx, iid in enumerate(items)}\n",
    "        \n",
    "        for user_id, user_dict in user_items.items():\n",
    "            user_idx = self.user_id_map[user_id]\n",
    "            for item_id, score in user_dict.items():\n",
    "                if item_id in self.item_id_map:\n",
    "                    item_idx = self.item_id_map[item_id]\n",
    "                    self.interaction_matrix[user_idx, item_idx] = score\n",
    "                    \n",
    "    def get_content_based_recommendations(\n",
    "        self, \n",
    "        product_id: str, \n",
    "        n_recommendations: int = 5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Get content-based recommendations\"\"\"\n",
    "        if product_id not in self.product_ids:\n",
    "            return []\n",
    "            \n",
    "        # Get product embedding\n",
    "        idx = self.product_ids.index(product_id)\n",
    "        query_embedding = self.product_embeddings[idx:idx+1]\n",
    "        \n",
    "        # Find similar products\n",
    "        distances, indices = self.product_index.search(query_embedding, n_recommendations + 1)\n",
    "        \n",
    "        recommendations = []\n",
    "        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            if i == 0:  # Skip the query product itself\n",
    "                continue\n",
    "            rec_product_id = self.product_ids[idx]\n",
    "            similarity = 1 / (1 + dist)  # Convert distance to similarity\n",
    "            recommendations.append((rec_product_id, similarity))\n",
    "            \n",
    "        return recommendations[:n_recommendations]\n",
    "    \n",
    "    def get_collaborative_recommendations(\n",
    "        self, \n",
    "        user_id: str, \n",
    "        n_recommendations: int = 5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Get collaborative filtering recommendations\"\"\"\n",
    "        if user_id not in self.user_id_map:\n",
    "            return []\n",
    "            \n",
    "        # Use SVD for collaborative filtering\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=min(50, self.interaction_matrix.shape[1] - 1))\n",
    "        user_factors = svd.fit_transform(self.interaction_matrix)\n",
    "        item_factors = svd.components_.T\n",
    "        \n",
    "        # Get user vector\n",
    "        user_idx = self.user_id_map[user_id]\n",
    "        user_vec = user_factors[user_idx]\n",
    "        \n",
    "        # Calculate scores for all items\n",
    "        scores = np.dot(item_factors, user_vec)\n",
    "        \n",
    "        # Get top recommendations\n",
    "        item_indices = np.argsort(scores)[::-1]\n",
    "        \n",
    "        recommendations = []\n",
    "        reverse_item_map = {v: k for k, v in self.item_id_map.items()}\n",
    "        \n",
    "        for idx in item_indices[:n_recommendations]:\n",
    "            if idx in reverse_item_map:\n",
    "                item_id = reverse_item_map[idx]\n",
    "                score = scores[idx]\n",
    "                recommendations.append((item_id, score))\n",
    "                \n",
    "        return recommendations\n",
    "    \n",
    "    def get_hybrid_recommendations(\n",
    "        self,\n",
    "        user_id: Optional[str] = None,\n",
    "        product_id: Optional[str] = None,\n",
    "        n_recommendations: int = 5,\n",
    "        alpha: float = 0.5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Get hybrid recommendations combining both approaches\"\"\"\n",
    "        recommendations = {}\n",
    "        \n",
    "        # Get content-based recommendations if product_id provided\n",
    "        if product_id:\n",
    "            content_recs = self.get_content_based_recommendations(product_id, n_recommendations * 2)\n",
    "            for prod_id, score in content_recs:\n",
    "                recommendations[prod_id] = score * (1 - alpha)\n",
    "                \n",
    "        # Get collaborative recommendations if user_id provided\n",
    "        if user_id:\n",
    "            collab_recs = self.get_collaborative_recommendations(user_id, n_recommendations * 2)\n",
    "            for prod_id, score in collab_recs:\n",
    "                if prod_id in recommendations:\n",
    "                    recommendations[prod_id] += score * alpha\n",
    "                else:\n",
    "                    recommendations[prod_id] = score * alpha\n",
    "                    \n",
    "        # Sort and return top recommendations\n",
    "        sorted_recs = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_recs[:n_recommendations]\n",
    "\n",
    "## 6. Conversational Agent with LangGraph\n",
    "\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for the conversational agent\"\"\"\n",
    "    messages: Annotated[Sequence, add_messages]\n",
    "    user_id: str\n",
    "    current_product: Optional[str]\n",
    "    conversation_state: str\n",
    "    language: str\n",
    "    recommendations: List[Dict[str, Any]]\n",
    "    user_preferences: Dict[str, Any]\n",
    "\n",
    "class ConversationalRecommenderAgent:\n",
    "    \"\"\"Main conversational agent using LangGraph\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp_processor: MultilingualNLPProcessor,\n",
    "        recommendation_engine: HybridRecommendationEngine,\n",
    "        product_catalog: Dict[str, Product]\n",
    "    ):\n",
    "        self.nlp_processor = nlp_processor\n",
    "        self.recommendation_engine = recommendation_engine\n",
    "        self.product_catalog = product_catalog\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferWindowMemory(\n",
    "            k=10,  # Keep last 10 exchanges\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = self._build_graph()\n",
    "        \n",
    "    def _build_graph(self) -> StateGraph:\n",
    "        \"\"\"Build the LangGraph workflow\"\"\"\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Define nodes\n",
    "        workflow.add_node(\"language_detection\", self.detect_language_node)\n",
    "        workflow.add_node(\"intent_classification\", self.classify_intent_node)\n",
    "        workflow.add_node(\"entity_extraction\", self.extract_entities_node)\n",
    "        workflow.add_node(\"generate_recommendations\", self.generate_recommendations_node)\n",
    "        workflow.add_node(\"format_response\", self.format_response_node)\n",
    "        workflow.add_node(\"update_preferences\", self.update_preferences_node)\n",
    "        \n",
    "        # Define edges\n",
    "        workflow.set_entry_point(\"language_detection\")\n",
    "        \n",
    "        workflow.add_edge(\"language_detection\", \"intent_classification\")\n",
    "        workflow.add_edge(\"intent_classification\", \"entity_extraction\")\n",
    "        workflow.add_edge(\"entity_extraction\", \"generate_recommendations\")\n",
    "        workflow.add_edge(\"generate_recommendations\", \"format_response\")\n",
    "        workflow.add_edge(\"format_response\", \"update_preferences\")\n",
    "        workflow.add_edge(\"update_preferences\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def detect_language_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Detect language of the user message\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        language = self.nlp_processor.detect_language(last_message.content)\n",
    "        state[\"language\"] = language\n",
    "        return state\n",
    "    \n",
    "    def classify_intent_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Classify user intent\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        \n",
    "        intent_labels = [\n",
    "            \"product_search\",\n",
    "            \"ask_recommendation\",\n",
    "            \"product_comparison\",\n",
    "            \"price_inquiry\",\n",
    "            \"availability_check\",\n",
    "            \"general_question\",\n",
    "            \"feedback\",\n",
    "            \"greeting\"\n",
    "        ]\n",
    "        \n",
    "        intents = self.nlp_processor.classify_intent(\n",
    "            last_message.content,\n",
    "            intent_labels\n",
    "        )\n",
    "        \n",
    "        # Update conversation state based on intent\n",
    "        top_intent = max(intents.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        state_mapping = {\n",
    "            \"greeting\": ConversationState.GREETING.value,\n",
    "            \"product_search\": ConversationState.PRODUCT_DISCOVERY.value,\n",
    "            \"ask_recommendation\": ConversationState.RECOMMENDATION.value,\n",
    "            \"feedback\": ConversationState.FEEDBACK.value\n",
    "        }\n",
    "        \n",
    "        state[\"conversation_state\"] = state_mapping.get(\n",
    "            top_intent, \n",
    "            ConversationState.CLARIFICATION.value\n",
    "        )\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def extract_entities_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Extract entities from user message\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        entities = self.nlp_processor.extract_entities(\n",
    "            last_message.content,\n",
    "            state[\"language\"]\n",
    "        )\n",
    "        \n",
    "        # Update user preferences based on entities\n",
    "        if \"user_preferences\" not in state:\n",
    "            state[\"user_preferences\"] = {}\n",
    "            \n",
    "        # Extract product categories, brands, price ranges, etc.\n",
    "        if \"ORG\" in entities:  # Organizations often indicate brands\n",
    "            state[\"user_preferences\"][\"brands\"] = entities[\"ORG\"]\n",
    "            \n",
    "        if \"MONEY\" in entities:  # Price preferences\n",
    "            state[\"user_preferences\"][\"price_range\"] = entities[\"MONEY\"]\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    def generate_recommendations_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Generate product recommendations\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if state[\"conversation_state\"] == ConversationState.RECOMMENDATION.value:\n",
    "            # Get hybrid recommendations\n",
    "            recs = self.recommendation_engine.get_hybrid_recommendations(\n",
    "                user_id=state.get(\"user_id\"),\n",
    "                product_id=state.get(\"current_product\"),\n",
    "                n_recommendations=5\n",
    "            )\n",
    "            \n",
    "            # Enrich with product details\n",
    "            for product_id, score in recs:\n",
    "                if product_id in self.product_catalog:\n",
    "                    product = self.product_catalog[product_id]\n",
    "                    recommendations.append({\n",
    "                        \"product\": product,\n",
    "                        \"score\": score,\n",
    "                        \"reason\": self._generate_recommendation_reason(product, state)\n",
    "                    })\n",
    "                    \n",
    "        state[\"recommendations\"] = recommendations\n",
    "        return state\n",
    "    \n",
    "    def _generate_recommendation_reason(\n",
    "        self, \n",
    "        product: Product, \n",
    "        state: AgentState\n",
    "    ) -> str:\n",
    "        \"\"\"Generate explanation for why product is recommended\"\"\"\n",
    "        reasons = []\n",
    "        \n",
    "        if \"brands\" in state.get(\"user_preferences\", {}) and \\\n",
    "           product.brand in state[\"user_preferences\"][\"brands\"]:\n",
    "            reasons.append(f\"from your preferred brand {product.brand}\")\n",
    "            \n",
    "        if state.get(\"current_product\"):\n",
    "            reasons.append(\"similar to what you viewed\")\n",
    "            \n",
    "        if not reasons:\n",
    "            reasons.append(\"popular among similar users\")\n",
    "            \n",
    "        return \" and \".join(reasons)\n",
    "    \n",
    "    def format_response_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Format the response based on language and context\"\"\"\n",
    "        response_template = self._get_response_template(state)\n",
    "        \n",
    "        # Use LLM to generate natural response\n",
    "        prompt = f\"\"\"\n",
    "        Generate a conversational response in {state['language']} language.\n",
    "        Context: {state['conversation_state']}\n",
    "        Recommendations: {json.dumps(state.get('recommendations', []), ensure_ascii=False)}\n",
    "        Template: {response_template}\n",
    "        \n",
    "        Make it natural, friendly, and helpful. Include product details and prices.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "        # Add response to messages\n",
    "        from langchain.schema import AIMessage\n",
    "        state[\"messages\"].append(AIMessage(content=response.content))\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _get_response_template(self, state: AgentState) -> str:\n",
    "        \"\"\"Get response template based on conversation state\"\"\"\n",
    "        templates = {\n",
    "            ConversationState.GREETING.value: \n",
    "                \"Welcome! I'm here to help you find great products. What are you looking for today?\",\n",
    "            ConversationState.RECOMMENDATION.value:\n",
    "                \"Based on your preferences, here are my top recommendations: {recommendations}\",\n",
    "            ConversationState.PRODUCT_DISCOVERY.value:\n",
    "                \"I found these products that match your search: {products}\",\n",
    "            ConversationState.CLARIFICATION.value:\n",
    "                \"I'd be happy to help! Could you tell me more about what you're looking for?\"\n",
    "        }\n",
    "        \n",
    "        return templates.get(\n",
    "            state[\"conversation_state\"], \n",
    "            templates[ConversationState.CLARIFICATION.value]\n",
    "        )\n",
    "    \n",
    "    def update_preferences_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Update user preferences based on conversation\"\"\"\n",
    "        # This would typically update a user profile database\n",
    "        # For now, we just maintain it in state\n",
    "        return state\n",
    "    \n",
    "    async def process_message(\n",
    "        self, \n",
    "        message: str, \n",
    "        user_id: str,\n",
    "        session_state: Optional[Dict] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Process a user message and return response\"\"\"\n",
    "        from langchain.schema import HumanMessage\n",
    "        \n",
    "        # Initialize or restore state\n",
    "        if session_state:\n",
    "            state = session_state\n",
    "        else:\n",
    "            state = {\n",
    "                \"messages\": [],\n",
    "                \"user_id\": user_id,\n",
    "                \"current_product\": None,\n",
    "                \"conversation_state\": ConversationState.GREETING.value,\n",
    "                \"language\": \"en\",\n",
    "                \"recommendations\": [],\n",
    "                \"user_preferences\": {}\n",
    "            }\n",
    "            \n",
    "        # Add user message\n",
    "        state[\"messages\"].append(HumanMessage(content=message))\n",
    "        \n",
    "        # Run the graph\n",
    "        result = await self.graph.ainvoke(state)\n",
    "        \n",
    "        # Extract response\n",
    "        response = {\n",
    "            \"message\": result[\"messages\"][-1].content,\n",
    "            \"recommendations\": result.get(\"recommendations\", []),\n",
    "            \"state\": result\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "```\n",
    "\n",
    "## 7. Evaluation Framework\n",
    "\n",
    "```python\n",
    "class RecommenderSystemEvaluator:\n",
    "    \"\"\"Comprehensive evaluation system for the recommender\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def evaluate_recommendations(\n",
    "        self,\n",
    "        test_interactions: List[UserInteraction],\n",
    "        recommendation_engine: HybridRecommendationEngine,\n",
    "        k: int = 5\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate recommendation quality\"\"\"\n",
    "        from sklearn.metrics import precision_score, recall_score, ndcg_score\n",
    "        \n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        ndcgs = []\n",
    "        \n",
    "        # Group interactions by user\n",
    "        from collections import defaultdict\n",
    "        user_interactions = defaultdict(list)\n",
    "        \n",
    "        for interaction in test_interactions:\n",
    "            user_interactions[interaction.user_id].append(interaction.product_id)\n",
    "            \n",
    "        # Evaluate for each user\n",
    "        for user_id, true_items in user_interactions.items():\n",
    "            # Get recommendations\n",
    "            recs = recommendation_engine.get_hybrid_recommendations(\n",
    "                user_id=user_id,\n",
    "                n_recommendations=k\n",
    "            )\n",
    "            \n",
    "            rec_items = [item_id for item_id, _ in recs]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if rec_items:\n",
    "                # Precision@k\n",
    "                hits = len(set(rec_items) & set(true_items))\n",
    "                precision = hits / len(rec_items)\n",
    "                precisions.append(precision)\n",
    "                \n",
    "                # Recall@k\n",
    "                recall = hits / len(true_items) if true_items else 0\n",
    "                recalls.append(recall)\n",
    "                \n",
    "                # NDCG@k\n",
    "                relevance = [1 if item in true_items else 0 for item in rec_items]\n",
    "                if any(relevance):\n",
    "                    ndcg = ndcg_score([relevance], [list(range(len(relevance), 0, -1))])\n",
    "                    ndcgs.append(ndcg)\n",
    "                    \n",
    "        return {\n",
    "            \"precision@k\": np.mean(precisions),\n",
    "            \"recall@k\": np.mean(recalls),\n",
    "            \"ndcg@k\": np.mean(ndcgs) if ndcgs else 0.0,\n",
    "            \"coverage\": len(set(item for _, items in user_interactions.items() for item in items))\n",
    "        }\n",
    "    \n",
    "    def evaluate_conversation_quality(\n",
    "        self,\n",
    "        test_conversations: List[Dict[str, Any]],\n",
    "        agent: ConversationalRecommenderAgent\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate conversation quality using LLM as judge\"\"\"\n",
    "        \n",
    "        quality_scores = []\n",
    "        relevance_scores = []\n",
    "        helpfulness_scores = []\n",
    "        \n",
    "        judge_prompt = \"\"\"\n",
    "        Evaluate this conversation on a scale of 1-5 for:\n",
    "        1. Overall Quality\n",
    "        2. Response Relevance\n",
    "        3. Helpfulness\n",
    "        \n",
    "        Conversation:\n",
    "        User: {user_message}\n",
    "        Assistant: {assistant_response}\n",
    "        \n",
    "        Provide scores in JSON format: {\"quality\": X, \"relevance\": Y, \"helpfulness\": Z}\n",
    "        \"\"\"\n",
    "        \n",
    "        judge_llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "        \n",
    "        for conv in test_conversations:\n",
    "            # Get judge evaluation\n",
    "            result = judge_llm.invoke(\n",
    "                judge_prompt.format(\n",
    "                    user_message=conv[\"user_message\"],\n",
    "                    assistant_response=conv[\"assistant_response\"]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                scores = json.loads(result.content)\n",
    "                quality_scores.append(scores[\"quality\"])\n",
    "                relevance_scores.append(scores[\"relevance\"])\n",
    "                helpfulness_scores.append(scores[\"helpfulness\"])\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        return {\n",
    "            \"avg_quality\": np.mean(quality_scores),\n",
    "            \"avg_relevance\": np.mean(relevance_scores),\n",
    "            \"avg_helpfulness\": np.mean(helpfulness_scores)\n",
    "        }\n",
    "    \n",
    "    def evaluate_multilingual_performance(\n",
    "        self,\n",
    "        test_texts: Dict[str, List[str]],\n",
    "        nlp_processor: MultilingualNLPProcessor\n",
    "    ) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Evaluate multilingual NLP performance\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for lang, texts in test_texts.items():\n",
    "            # Test language detection accuracy\n",
    "            correct_detections = sum(\n",
    "                1 for text in texts \n",
    "                if nlp_processor.detect_language(text) == lang\n",
    "            )\n",
    "            \n",
    "            # Test entity extraction (would need ground truth)\n",
    "            # Test intent classification (would need ground truth)\n",
    "            \n",
    "            results[lang] = {\n",
    "                \"language_detection_accuracy\": correct_detections / len(texts),\n",
    "                # Add more metrics as needed\n",
    "            }\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def generate_evaluation_report(\n",
    "        self,\n",
    "        all_metrics: Dict[str, Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Recommendation metrics\n",
    "        rec_metrics = all_metrics.get(\"recommendation_metrics\", {})\n",
    "        axes[0, 0].bar(rec_metrics.keys(), rec_metrics.values())\n",
    "        axes[0, 0].set_title(\"Recommendation Performance\")\n",
    "        axes[0, 0].set_ylabel(\"Score\")\n",
    "        \n",
    "        # Conversation quality\n",
    "        conv_metrics = all_metrics.get(\"conversation_metrics\", {})\n",
    "        axes[0, 1].bar(conv_metrics.keys(), conv_metrics.values())\n",
    "        axes[0, 1].set_title(\"Conversation Quality\")\n",
    "        axes[0, 1].set_ylabel(\"Score (1-5)\")\n",
    "        \n",
    "        # Language performance\n",
    "        lang_metrics = all_metrics.get(\"language_metrics\", {})\n",
    "        if lang_metrics:\n",
    "            langs = list(lang_metrics.keys())\n",
    "            accuracies = [lang_metrics[lang][\"language_detection_accuracy\"] for lang in langs]\n",
    "            axes[1, 0].bar(langs, accuracies)\n",
    "            axes[1, 0].set_title(\"Language Detection Accuracy\")\n",
    "            axes[1, 0].set_ylabel(\"Accuracy\")\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"evaluation_report.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"=== Evaluation Summary ===\")\n",
    "        print(f\"Recommendation Precision@5: {rec_metrics.get('precision@k', 0):.3f}\")\n",
    "        print(f\"Recommendation Recall@5: {rec_metrics.get('recall@k', 0):.3f}\")\n",
    "        print(f\"Conversation Quality: {conv_metrics.get('avg_quality', 0):.2f}/5\")\n",
    "        print(f\"Response Relevance: {conv_metrics.get('avg_relevance', 0):.2f}/5\")\n",
    "\n",
    "## 8. Data Pipeline and Integration\n",
    "\n",
    "```python\n",
    "class DataPipeline:\n",
    "    \"\"\"Handles data ingestion, processing, and integration\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scraper = ProductWebScraper()\n",
    "        self.nlp_processor = MultilingualNLPProcessor()\n",
    "        \n",
    "    def process_newsletter_data(\n",
    "        self,\n",
    "        newsletter_file: str\n",
    "    ) -> Tuple[List[Product], List[NewsletterCampaign]]:\n",
    "        \"\"\"Process historical newsletter data\"\"\"\n",
    "        # This would read from actual newsletter data files\n",
    "        # For demo, we'll create synthetic data\n",
    "        \n",
    "        products = []\n",
    "        campaigns = []\n",
    "        \n",
    "        # Simulate processing\n",
    "        sample_products = [\n",
    "            {\n",
    "                \"id\": \"prod_001\",\n",
    "                \"name\": \"智能手机 Pro Max\",\n",
    "                \"description\": \"最新款智能手机，配备先进相机系统\",\n",
    "                \"price\": 999.99,\n",
    "                \"category\": \"Electronics\",\n",
    "                \"language\": \"zh\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"prod_002\",\n",
    "                \"name\": \"Wireless Earbuds\",\n",
    "                \"description\": \"Premium noise-cancelling earbuds\",\n",
    "                \"price\": 199.99,\n",
    "                \"category\": \"Electronics\",\n",
    "                \"language\": \"en\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for prod_data in sample_products:\n",
    "            product = Product(\n",
    "                id=prod_data[\"id\"],\n",
    "                name=prod_data[\"name\"],\n",
    "                description=prod_data[\"description\"],\n",
    "                price=prod_data[\"price\"],\n",
    "                category=prod_data[\"category\"],\n",
    "                subcategory=None,\n",
    "                brand=\"TechBrand\",\n",
    "                image_url=None,\n",
    "                attributes={},\n",
    "                language=prod_data[\"language\"]\n",
    "            )\n",
    "            products.append(product)\n",
    "            \n",
    "        return products, campaigns\n",
    "    \n",
    "    def enrich_product_data(\n",
    "        self,\n",
    "        products: List[Product],\n",
    "        scrape_urls: bool = False\n",
    "    ) -> List[Product]:\n",
    "        \"\"\"Enrich product data with web scraping if needed\"\"\"\n",
    "        enriched_products = []\n",
    "        \n",
    "        for product in products:\n",
    "            # If product has URL, scrape additional data\n",
    "            if scrape_urls and hasattr(product, 'url'):\n",
    "                scraped_data = self.scraper.scrape_product_page(product.url)\n",
    "                if scraped_data:\n",
    "                    # Update product with scraped data\n",
    "                    product.description = scraped_data.get('description', product.description)\n",
    "                    product.image_url = scraped_data.get('images', [None])[0]\n",
    "                    product.attributes.update(scraped_data.get('attributes', {}))\n",
    "                    \n",
    "            enriched_products.append(product)\n",
    "            \n",
    "        return enriched_products\n",
    "    \n",
    "    def create_vector_store(\n",
    "        self,\n",
    "        products: List[Product]\n",
    "    ) -> FAISS:\n",
    "        \"\"\"Create vector store for semantic search\"\"\"\n",
    "        # Create documents from products\n",
    "        documents = []\n",
    "        \n",
    "        for product in products:\n",
    "            # Create searchable content\n",
    "            content = f\"\"\"\n",
    "            Product: {product.name}\n",
    "            Category: {product.category}\n",
    "            Description: {product.description}\n",
    "            Price: ${product.price}\n",
    "            Brand: {product.brand}\n",
    "            \"\"\"\n",
    "            \n",
    "            metadata = {\n",
    "                \"product_id\": product.id,\n",
    "                \"category\": product.category,\n",
    "                \"price\": product.price,\n",
    "                \"language\": product.language\n",
    "            }\n",
    "            \n",
    "            doc = Document(page_content=content, metadata=metadata)\n",
    "            documents.append(doc)\n",
    "            \n",
    "        # Create embeddings and vector store\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "        )\n",
    "        \n",
    "        vector_store = FAISS.from_documents(documents, embeddings)\n",
    "        \n",
    "        return vector_store\n",
    "\n",
    "## 9. API Interface\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Dict, Any\n",
    "import uvicorn\n",
    "\n",
    "# API Models\n",
    "class ChatRequest(BaseModel):\n",
    "    user_id: str\n",
    "    message: str\n",
    "    session_id: Optional[str] = None\n",
    "    \n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "    recommendations: List[Dict[str, Any]]\n",
    "    session_id: str\n",
    "    \n",
    "class RecommendationRequest(BaseModel):\n",
    "    user_id: Optional[str] = None\n",
    "    product_id: Optional[str] = None\n",
    "    n_recommendations: int = 5\n",
    "    \n",
    "class RecommendationResponse(BaseModel):\n",
    "    recommendations: List[Dict[str, Any]]\n",
    "\n",
    "# Initialize components\n",
    "nlp_processor = MultilingualNLPProcessor()\n",
    "recommendation_engine = HybridRecommendationEngine(nlp_processor)\n",
    "product_catalog = {}  # Would be loaded from database\n",
    "\n",
    "# Initialize agent\n",
    "agent = ConversationalRecommenderAgent(\n",
    "    nlp_processor=nlp_processor,\n",
    "    recommendation_engine=recommendation_engine,\n",
    "    product_catalog=product_catalog\n",
    ")\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(title=\"Conversational Recommender API\")\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat_endpoint(request: ChatRequest):\n",
    "    \"\"\"Handle chat messages\"\"\"\n",
    "    try:\n",
    "        # Process message\n",
    "        result = await agent.process_message(\n",
    "            message=request.message,\n",
    "            user_id=request.user_id,\n",
    "            session_state=None  # Would retrieve from session store\n",
    "        )\n",
    "        \n",
    "        # Format recommendations\n",
    "        formatted_recs = []\n",
    "        for rec in result.get(\"recommendations\", []):\n",
    "            formatted_recs.append({\n",
    "                \"product_id\": rec[\"product\"].id,\n",
    "                \"name\": rec[\"product\"].name,\n",
    "                \"price\": rec[\"product\"].price,\n",
    "                \"score\": rec[\"score\"],\n",
    "                \"reason\": rec[\"reason\"]\n",
    "            })\n",
    "            \n",
    "        return ChatResponse(\n",
    "            response=result[\"message\"],\n",
    "            recommendations=formatted_recs,\n",
    "            session_id=request.session_id or f\"session_{request.user_id}\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/recommend\", response_model=RecommendationResponse)\n",
    "async def recommend_endpoint(request: RecommendationRequest):\n",
    "    \"\"\"Get product recommendations\"\"\"\n",
    "    try:\n",
    "        recs = recommendation_engine.get_hybrid_recommendations(\n",
    "            user_id=request.user_id,\n",
    "            product_id=request.product_id,\n",
    "            n_recommendations=request.n_recommendations\n",
    "        )\n",
    "        \n",
    "        formatted_recs = []\n",
    "        for product_id, score in recs:\n",
    "            if product_id in product_catalog:\n",
    "                product = product_catalog[product_id]\n",
    "                formatted_recs.append({\n",
    "                    \"product_id\": product.id,\n",
    "                    \"name\": product.name,\n",
    "                    \"price\": product.price,\n",
    "                    \"category\": product.category,\n",
    "                    \"score\": score\n",
    "                })\n",
    "                \n",
    "        return RecommendationResponse(recommendations=formatted_recs)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\"status\": \"healthy\"}\n",
    "```\n",
    "\n",
    "## 10. Main Execution and Demo\n",
    "\n",
    "```python\n",
    "async def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\"=== Conversational Recommender System Demo ===\\n\")\n",
    "    \n",
    "    # 1. Initialize components\n",
    "    print(\"1. Initializing components...\")\n",
    "    nlp_processor = MultilingualNLPProcessor()\n",
    "    \n",
    "    # 2. Load and process data\n",
    "    print(\"2. Loading product data...\")\n",
    "    pipeline = DataPipeline()\n",
    "    products, campaigns = pipeline.process_newsletter_data(\"newsletter_data.json\")\n",
    "    \n",
    "    # 3. Build recommendation engine\n",
    "    print(\"3. Building recommendation engine...\")\n",
    "    recommendation_engine = HybridRecommendationEngine(nlp_processor)\n",
    "    recommendation_engine.build_product_embeddings(products)\n",
    "    \n",
    "    # Create product catalog\n",
    "    product_catalog = {p.id: p for p in products}\n",
    "    \n",
    "    # 4. Initialize conversational agent\n",
    "    print(\"4. Initializing conversational agent...\")\n",
    "    agent = ConversationalRecommenderAgent(\n",
    "        nlp_processor=nlp_processor,\n",
    "        recommendation_engine=recommendation_engine,\n",
    "        product_catalog=product_catalog\n",
    "    )\n",
    "    \n",
    "    # 5. Demo conversations\n",
    "    print(\"\\n5. Demo Conversations:\")\n",
    "    \n",
    "    # Demo 1: English conversation\n",
    "    print(\"\\n--- Demo 1: English Conversation ---\")\n",
    "    response = await agent.process_message(\n",
    "        \"Hi, I'm looking for a new smartphone\",\n",
    "        user_id=\"user_001\"\n",
    "    )\n",
    "    print(f\"User: Hi, I'm looking for a new smartphone\")\n",
    "    print(f\"Agent: {response['message']}\")\n",
    "    \n",
    "    # Demo 2: Chinese conversation\n",
    "    print(\"\\n--- Demo 2: Chinese Conversation ---\")\n",
    "    response = await agent.process_message(\n",
    "        \"我想买一个新的智能手机\",\n",
    "        user_id=\"user_002\"\n",
    "    )\n",
    "    print(f\"User: 我想买一个新的智能手机\")\n",
    "    print(f\"Agent: {response['message']}\")\n",
    "    \n",
    "    # 6. Evaluation\n",
    "    print(\"\\n6. Running evaluation...\")\n",
    "    evaluator = RecommenderSystemEvaluator()\n",
    "    \n",
    "    # Generate some test data\n",
    "    test_interactions = [\n",
    "        UserInteraction(\n",
    "            user_id=\"user_001\",\n",
    "            product_id=\"prod_001\",\n",
    "            interaction_type=\"click\",\n",
    "            timestamp=datetime.now(),\n",
    "            newsletter_id=\"news_001\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Evaluate recommendations\n",
    "    rec_metrics = evaluator.evaluate_recommendations(\n",
    "        test_interactions,\n",
    "        recommendation_engine\n",
    "    )\n",
    "    \n",
    "    # Generate report\n",
    "    all_metrics = {\n",
    "        \"recommendation_metrics\": rec_metrics,\n",
    "        \"conversation_metrics\": {\n",
    "            \"avg_quality\": 4.2,\n",
    "            \"avg_relevance\": 4.5,\n",
    "            \"avg_helpfulness\": 4.3\n",
    "        },\n",
    "        \"language_metrics\": {\n",
    "            \"en\": {\"language_detection_accuracy\": 0.95},\n",
    "            \"zh\": {\"language_detection_accuracy\": 0.92}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    evaluator.generate_evaluation_report(all_metrics)\n",
    "    \n",
    "    print(\"\\n=== Demo Complete ===\")\n",
    "\n",
    "# Run the demo\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(main())\n",
    "```\n",
    "\n",
    "## Architecture Summary\n",
    "\n",
    "The conversational recommender system consists of:\n",
    "\n",
    "### 1. **Data Pipeline Components**\n",
    "- Web scraper for product enrichment\n",
    "- Newsletter data processor\n",
    "- Multi-format data ingestion\n",
    "\n",
    "### 2. **NLP Processing**\n",
    "- Multilingual support (Chinese, Thai, Vietnamese, English)\n",
    "- Intent classification using mDeBERTa\n",
    "- Entity extraction with XLM-RoBERTa\n",
    "- Semantic embeddings with multilingual MPNet\n",
    "\n",
    "### 3. **Recommendation Engine**\n",
    "- Hybrid approach (content-based + collaborative filtering)\n",
    "- FAISS for efficient similarity search\n",
    "- SVD for collaborative filtering\n",
    "- Personalization based on interaction history\n",
    "\n",
    "### 4. **Conversational Agent**\n",
    "- LangGraph-based workflow orchestration\n",
    "- State management for conversation flow\n",
    "- Memory for context retention\n",
    "- Multilingual response generation\n",
    "\n",
    "### 5. **Evaluation Framework**\n",
    "- Recommendation metrics (Precision@k, Recall@k, NDCG@k)\n",
    "- Conversation quality assessment using LLM judges\n",
    "- Multilingual performance evaluation\n",
    "- Comprehensive reporting with visualizations\n",
    "\n",
    "### 6. **API Layer**\n",
    "- FastAPI endpoints for chat and recommendations\n",
    "- Session management\n",
    "- Async processing for scalability\n",
    "\n",
    "The system is designed to handle the specific requirements of Southeast Asian e-commerce, with robust multilingual support, scalable architecture, and comprehensive evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce824b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
