{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.embeddings import DeterministicFakeEmbedding\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43b9a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # loads ANTHROPIC_API_KEY as an environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92fcd310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example user data\n",
    "user_profiles = {\n",
    "    \"john\": {\"name\": \"John\", \"nationality\": \"US\", \"language\": \"English\"},\n",
    "    \"wei\": {\"name\": \"Wei\", \"nationality\": \"CN\", \"language\": \"Chinese\"},\n",
    "    \"tanaka\": {\"name\": \"Tanaka\", \"nationality\": \"JP\", \"language\": \"Japanese\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b45a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example product catalog (in-memory)\n",
    "products = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"Wireless Earbuds\",\n",
    "        \"category\": \"Electronics\",\n",
    "        \"price\": 59.99,\n",
    "        \"image_url\": \"http://example.com/earbuds.jpg\",\n",
    "        \"product_url\": \"http://example.com/earbuds\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"Fitness Watch\",\n",
    "        \"category\": \"Electronics\",\n",
    "        \"price\": 129.99,\n",
    "        \"image_url\": \"http://example.com/watch.jpg\",\n",
    "        \"product_url\": \"http://example.com/watch\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"Coffee Mug\",\n",
    "        \"category\": \"Kitchen\",\n",
    "        \"price\": 12.50,\n",
    "        \"image_url\": \"http://example.com/mug.jpg\",\n",
    "        \"product_url\": \"http://example.com/mug\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c92d5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings and vector stores for marketed products\n",
    "descriptions = [\n",
    "    p[\"name\"] + \" \" + p[\"category\"] + \" \" + str(p[\"price\"]) for p in products\n",
    "]\n",
    "embeddings = DeterministicFakeEmbedding(size=4096)\n",
    "vectors = embeddings.embed_documents(descriptions)\n",
    "store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9985b630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b259dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0e526b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf91630c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(model='llama3', base_url=None, client_kwargs={}, async_client_kwargs={}, sync_client_kwargs={}, mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, keep_alive=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fe5706c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescriptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/playground/e-comm/.venv/lib/python3.12/site-packages/langchain_ollama/embeddings.py:265\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    264\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed search docs.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     embedded_docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embedded_docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/playground/e-comm/.venv/lib/python3.12/site-packages/ollama/_client.py:367\u001b[39m, in \u001b[36mClient.embed\u001b[39m\u001b[34m(self, model, input, truncate, options, keep_alive)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed\u001b[39m(\n\u001b[32m    360\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    361\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    365\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    366\u001b[39m ) -> EmbedResponse:\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mEmbedResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/embed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEmbedRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/playground/e-comm/.venv/lib/python3.12/site-packages/ollama/_client.py:180\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/playground/e-comm/.venv/lib/python3.12/site-packages/ollama/_client.py:126\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "source": [
    "embeddings.embed_documents(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c88eb25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_product(query: str) -> str:\n",
    "    \"\"\"Return top product matching the query from vector store.\"\"\"\n",
    "    docs = store.similarity_search(query, k=1)\n",
    "    if docs:\n",
    "        prod = docs[0]  # top result\n",
    "        return f\"Found product: {prod['name']} (${prod['price']}).\"\n",
    "    return \"No matching products found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2695d976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No matching products found.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_product(\"headphones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc043e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wireless Earbuds Electronics 59.99',\n",
       " 'Fitness Watch Electronics 129.99',\n",
       " 'Coffee Mug Kitchen 12.5']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = InMemoryVectorStore.from_vectors(doc_vectors, documents=products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd46168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "351fdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"The schema class that defines the state.\"\"\"\n",
    "\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118c4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure the chatbot as a \"state machine\" given the above State class\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5206706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a ChatModel class\n",
    "llm = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State) -> dict[str, list[AIMessage]]:\n",
    "    \"\"\"Return a structures response from the LLM.\"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f27ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    graph_builder.add_node(\"chatbot\", chatbot)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    graph_builder.add_edge(START, \"chatbot\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c6c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8bd0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9382253",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "):\n",
    "    for value in event.values():\n",
    "        print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(state: dict) -> dict:\n",
    "    \"\"\"Return a language preference given the state.\"\"\"\n",
    "    user_id = state.get(\"user_id\")\n",
    "    profile = user_profiles.get(user_id, {})\n",
    "    language = profile.get(\"language\", \"English\")  # default to English\n",
    "    return {\"language\": language}  # update state with detected language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63257053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # add the detect_language node in the graph\n",
    "    workflow.add_node(\"detect_language\", detect_language)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # add the route from the Start to the detect_lanugage node\n",
    "    workflow.add_edge(START, \"detect_language\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_route(state: dict) -> str:\n",
    "    return (\n",
    "        \"greet_chinese\"\n",
    "        if state.get(\"language\") == \"Chinese\"\n",
    "        else \"greet_English\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b06281",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_conditional_edges(\"detect_language\", language_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f435ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet_english(state: dict):\n",
    "    name = user_profiles[state[\"user_id\"]][\"name\"]\n",
    "    msg = f\"Hello {name}! How can I assist you today?\"\n",
    "    # Append to conversation log\n",
    "    conv = state.get(\"conversation\", [])\n",
    "    return {\"conversation\": conv + [{\"role\": \"assistant\", \"content\": msg}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8047cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet_chinese(state: dict):\n",
    "    name = user_profiles[state[\"user_id\"]][\"name\"]\n",
    "    # Simple Chinese greeting\n",
    "    msg = f\"你好 {name}！请问今天需要什么帮助？\"\n",
    "    conv = state.get(\"conversation\", [])\n",
    "    return {\"conversation\": conv + [{\"role\": \"assistant\", \"content\": msg}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"greet_english\", greet_english)\n",
    "workflow.add_node(\"greet_chinese\", greet_chinese)\n",
    "workflow.add_edge(\"greet_english\", END)\n",
    "workflow.add_edge(\"greet_chinese\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840dea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c2a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
